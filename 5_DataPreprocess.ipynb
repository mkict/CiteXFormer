{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PvlCPRx5TtuEE66bsKsRwaojm1XyPGun","timestamp":1701225950143}],"toc_visible":true,"authorship_tag":"ABX9TyPtucnKsNUWBs963bXP+zDm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"625be81f804b43b78617791ce44dcdc1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05df3460751641daad28321af1ea67b1","IPY_MODEL_3e35a2c378554fb3bc4e80fa00aedfaf","IPY_MODEL_92faea01d91d40edbd5dd469f94648eb"],"layout":"IPY_MODEL_f15b65ed26314601abe1a99c609de89a"}},"05df3460751641daad28321af1ea67b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7ad821109ce4c0a86e225f5daff8882","placeholder":"​","style":"IPY_MODEL_e8e981cdd8b54c62aa25c112e9c6c7f7","value":"100%"}},"3e35a2c378554fb3bc4e80fa00aedfaf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecec38339a744b3aa0f50008283aa91e","max":30,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a93da34c97c443f198f6b9aa1845741f","value":30}},"92faea01d91d40edbd5dd469f94648eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2a2c0f826564dac923838afe64d03de","placeholder":"​","style":"IPY_MODEL_4b2ba9d3a4ed4f4e93f7a25b0092344f","value":" 30/30 [06:26&lt;00:00, 12.96s/it]"}},"f15b65ed26314601abe1a99c609de89a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7ad821109ce4c0a86e225f5daff8882":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8e981cdd8b54c62aa25c112e9c6c7f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecec38339a744b3aa0f50008283aa91e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a93da34c97c443f198f6b9aa1845741f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a2a2c0f826564dac923838afe64d03de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b2ba9d3a4ed4f4e93f7a25b0092344f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea1765f1082c4ec79fa449b9a563c6b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_378eb620ee6c4261888ecd6f5ec2f7af","IPY_MODEL_10771ebd60c94ab6a0bb5d1751d1efe6","IPY_MODEL_7e6c860fbe4d4aa89c1fd7d2136f46d5"],"layout":"IPY_MODEL_685e79d3577e403ca49c4e756dc54ec3"}},"378eb620ee6c4261888ecd6f5ec2f7af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_034d582f6c504ddf9846693517c4f9ec","placeholder":"​","style":"IPY_MODEL_ddd3194014a84405b02e0600f4fe0b95","value":"100%"}},"10771ebd60c94ab6a0bb5d1751d1efe6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f83add10bb87464db710282701f5681f","max":171123,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f2a1083d6fb4fa58121229ae0055e7c","value":171123}},"7e6c860fbe4d4aa89c1fd7d2136f46d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5da3968c386645e8b62b425744401318","placeholder":"​","style":"IPY_MODEL_ea6821f4ad064171ba9d3a2cfae3c53f","value":" 171123/171123 [09:04&lt;00:00, 396.39it/s]"}},"685e79d3577e403ca49c4e756dc54ec3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"034d582f6c504ddf9846693517c4f9ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd3194014a84405b02e0600f4fe0b95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f83add10bb87464db710282701f5681f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f2a1083d6fb4fa58121229ae0055e7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5da3968c386645e8b62b425744401318":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea6821f4ad064171ba9d3a2cfae3c53f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import re\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# dataset location\n","base_path = \"/content/drive/Shareddrives/NLP Fall 2023/Dataset/\"\n","\n","# mapping logic function to normalize header into [introduction, method, result, conslusion]\n","def normalize_header(header):\n","  header = str(header).lower()\n","  if bool(re.search(r'introduction|background|related? work?', header)):\n","    return \"introduction\"\n","  if bool(re.search(r'experiment|evaluat|result', header)):\n","    return \"result\"\n","  if bool(re.search(r'method|solution', header)):\n","    return \"method\"\n","  if bool(re.search(r'conclusion|discussion|summar|conclud', header)):\n","    return \"conclusion\"\n","\n","  if bool(re.search(r'overview|problem|motivation|prelim', header)):\n","    return \"introduction\"\n","  return \"other\""],"metadata":{"id":"h-gSEBN3gRzf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701228396464,"user_tz":-420,"elapsed":7102,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"1152b3e6-8ee5-4b65-e225-1e067d536d78"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["### Dataset schema exploration"],"metadata":{"id":"6TnyAlxttoo6"}},{"cell_type":"code","source":["dataset = pd.read_json(base_path + 'filter/dataset_1.json')\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"iPsXGkJ0s7G6","executionInfo":{"status":"ok","timestamp":1701227067860,"user_tz":-420,"elapsed":9457,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"7d13b340-0a58-446b-a3be-bd6ff733fb39"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       corpusid                                        externalids  \\\n","0      13292366  {'arxiv': '1609.04186', 'mag': '2962834107', '...   \n","1      20527197  {'arxiv': '1705.06821', 'mag': '2982426383', '...   \n","2        116505  {'arxiv': '1711.02536', 'mag': '2963233928', '...   \n","3       4693065  {'arxiv': '1804.02199', 'mag': '2963695770', '...   \n","4      85498398  {'arxiv': '1903.10195', 'mag': '2961568745', '...   \n","...         ...                                                ...   \n","5851  244918896  {'arxiv': None, 'mag': None, 'acl': None, 'pub...   \n","5852  246649327  {'arxiv': None, 'mag': None, 'acl': None, 'pub...   \n","5853  225830257  {'arxiv': None, 'mag': '3043116244', 'acl': No...   \n","5854  202772449  {'arxiv': None, 'mag': '2972235673', 'acl': No...   \n","5855  212964741  {'arxiv': None, 'mag': '2999564766', 'acl': No...   \n","\n","                                                content  \n","0     {'source': {'pdfurls': ['https://www.aclweb.or...  \n","1     {'source': {'pdfurls': ['https://arxiv.org/pdf...  \n","2     {'source': {'pdfurls': ['https://arxiv.org/pdf...  \n","3     {'source': {'pdfurls': ['https://arxiv.org/pdf...  \n","4     {'source': {'pdfurls': ['https://arxiv.org/pdf...  \n","...                                                 ...  \n","5851  {'source': {'pdfurls': None, 'pdfsha': '43d177...  \n","5852  {'source': {'pdfurls': None, 'pdfsha': 'e0dd44...  \n","5853  {'source': {'pdfurls': None, 'pdfsha': '9fda88...  \n","5854  {'source': {'pdfurls': ['https://web.archive.o...  \n","5855  {'source': {'pdfurls': ['https://web.archive.o...  \n","\n","[5856 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-23cb4df1-7d5e-4519-b3ce-3b43d7ebba1a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corpusid</th>\n","      <th>externalids</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13292366</td>\n","      <td>{'arxiv': '1609.04186', 'mag': '2962834107', '...</td>\n","      <td>{'source': {'pdfurls': ['https://www.aclweb.or...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20527197</td>\n","      <td>{'arxiv': '1705.06821', 'mag': '2982426383', '...</td>\n","      <td>{'source': {'pdfurls': ['https://arxiv.org/pdf...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>116505</td>\n","      <td>{'arxiv': '1711.02536', 'mag': '2963233928', '...</td>\n","      <td>{'source': {'pdfurls': ['https://arxiv.org/pdf...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4693065</td>\n","      <td>{'arxiv': '1804.02199', 'mag': '2963695770', '...</td>\n","      <td>{'source': {'pdfurls': ['https://arxiv.org/pdf...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>85498398</td>\n","      <td>{'arxiv': '1903.10195', 'mag': '2961568745', '...</td>\n","      <td>{'source': {'pdfurls': ['https://arxiv.org/pdf...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5851</th>\n","      <td>244918896</td>\n","      <td>{'arxiv': None, 'mag': None, 'acl': None, 'pub...</td>\n","      <td>{'source': {'pdfurls': None, 'pdfsha': '43d177...</td>\n","    </tr>\n","    <tr>\n","      <th>5852</th>\n","      <td>246649327</td>\n","      <td>{'arxiv': None, 'mag': None, 'acl': None, 'pub...</td>\n","      <td>{'source': {'pdfurls': None, 'pdfsha': 'e0dd44...</td>\n","    </tr>\n","    <tr>\n","      <th>5853</th>\n","      <td>225830257</td>\n","      <td>{'arxiv': None, 'mag': '3043116244', 'acl': No...</td>\n","      <td>{'source': {'pdfurls': None, 'pdfsha': '9fda88...</td>\n","    </tr>\n","    <tr>\n","      <th>5854</th>\n","      <td>202772449</td>\n","      <td>{'arxiv': None, 'mag': '2972235673', 'acl': No...</td>\n","      <td>{'source': {'pdfurls': ['https://web.archive.o...</td>\n","    </tr>\n","    <tr>\n","      <th>5855</th>\n","      <td>212964741</td>\n","      <td>{'arxiv': None, 'mag': '2999564766', 'acl': No...</td>\n","      <td>{'source': {'pdfurls': ['https://web.archive.o...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5856 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23cb4df1-7d5e-4519-b3ce-3b43d7ebba1a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-23cb4df1-7d5e-4519-b3ce-3b43d7ebba1a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-23cb4df1-7d5e-4519-b3ce-3b43d7ebba1a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ec21892c-585b-48a6-9ef6-f5497af975cc\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec21892c-585b-48a6-9ef6-f5497af975cc')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ec21892c-585b-48a6-9ef6-f5497af975cc button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["idx = 0\n","dataset['externalids'][idx]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3z9okXczBQi","executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":36,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"a379bbe5-1ba8-46b4-d052-8433eae86707"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'arxiv': '1609.04186',\n"," 'mag': '2962834107',\n"," 'acl': 'C16-1291',\n"," 'pubmed': None,\n"," 'pubmedcentral': None,\n"," 'dblp': 'conf/coling/LiuUFS16',\n"," 'doi': None}"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["dataset['content'][idx]"],"metadata":{"id":"S0zreMSwxikE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":33,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"1d06100f-e7f0-46bf-f9d3-448e387322d7"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'source': {'pdfurls': ['https://www.aclweb.org/anthology/C16-1291.pdf'],\n","  'pdfsha': '7f12bd8efc6791399abdc587a1ca4f52776e2b88',\n","  'oainfo': None},\n"," 'text': \"\\nNeural Machine Translation with Supervised Attention\\nDecember 11-17 2016\\n\\nLemao Liu lmliu@nict.go.jp \\nNICT)\\nNational Institute of Information and Communications Technology\\n3-5 Hikari-dai, Seika-cho, Soraku-gunKyotoJapan\\n\\nMasao Utiyama \\nNICT)\\nNational Institute of Information and Communications Technology\\n3-5 Hikari-dai, Seika-cho, Soraku-gunKyotoJapan\\n\\nAndrew Finch \\nNICT)\\nNational Institute of Information and Communications Technology\\n3-5 Hikari-dai, Seika-cho, Soraku-gunKyotoJapan\\n\\nEiichiro Sumita \\nNICT)\\nNational Institute of Information and Communications Technology\\n3-5 Hikari-dai, Seika-cho, Soraku-gunKyotoJapan\\n\\nNeural Machine Translation with Supervised Attention\\n\\nProceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers\\nCOLING 2016, the 26th International Conference on Computational Linguistics: Technical PapersOsaka, JapanDecember 11-17 2016\\nThe attention mechanism is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in alignment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.This work is licensed under a Creative Commons Attribution 4.0 International Licence.Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Throughout this paper, without the special statement, NMT means attention-based NMT. 2  We do agree that NMT is a supervised model with respect to translation rather than reordering.\\n\\nIntroduction\\n\\nNeural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015;Sutskever et al., 2015). Generally, it relies on a recurrent neural network under the Encode-Decode framework: it firstly encodes a source sentence into context vectors and then generates its translation token-by-token, selecting from the target vocabulary. Among different variants of NMT, attention based NMT, which is the focus of this paper, 1 is attracting increasing interests in the community (Bahdanau et al., 2015;. One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance.\\n\\nAn attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000;Dyer et al., 2013;Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment. 2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006;Koehn et al., 2007;Bisazza and Federico, 2016).\\n\\nInspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the conventional aligners delivers higher quality alignment, it is expected that the alignment in the supervised attention NMT will be improved leading to better end-to-end translation performance. One advantage of the proposed SA-NMT is that it implements the supervision of attention as a regularization in the joint training objective ( §3.2). Furthermore, since the attention variables lies in the middle of the entire network architecture rather than the top (as the translation variables (see Figure 1(b)), it serves to mitigate the vanishing gradient problem during the back-propagation, by adding supervision into the intermediate layers in the network (Szegedy et al., 2015). This paper makes the following contributions:\\n\\n• It revisits the attention model from the point view of reordering ( §2), and propose a supervised attention for NMT that is supervised by statistical alignment models ( §3). The proposed approach is simple and easy to be implemented, and it is generally applicable to any attention-based NMT models, although in this case it is implemented on top of the model in (Bahdanau et al., 2015).\\n\\n• On two Chinese-to-English translation tasks, it empirically shows that the proposed approach gives rise to improved performance ( §4): on a large scale task, it outperforms three baselines including a state-of-the-art Moses, and leads to improvements of up to 2.5 BLEU points over the strongest one in this paper; on a low resource task, it even obtains about 5 BLEU points over the attention based NMT system on which is it based. Suppose x = x 1 , x 2 , · · · , x m denotes a source sentence, y = y 1 , y 2 , · · · , y n a target sentence. In addition, let x <t = x 1 , x 2 , · · · , x t−1 denote a prefix of x. Neural Machine Translation (NMT) directly maps a source sentence into a target under an encode-decode framework. In the encoding stage, it uses two bidirectional recurrent neural networks to encode x into a sequence of vectors E x = E x 1 , E x 2 , · · · , E xm , with E x i representing the concatenation of two vectors for i th source word from two directional RNNs. In the decoding stage, it generates the target translation from the conditional probability over the pair of sequences x and y via a recurrent neural network parametrized by θ as follows:\\n\\n\\nRevisiting Neural Machine Translation\\nht ht 1 ↵ t c t y t 1 y t ht ht 1 c t y t 1 y t ↵ t E x E x ht h t 1 ↵ t c t y t 1 y t ht h t 1 c t y t 1 y t ↵ t E x E xp(y | x; θ) = n t=1 p(y t | y <t , E x ) = n t=1 softmax g(y t−1 , h t , c t ) [y t ](1)\\nwhere h t and c t respectively denote an RNN hidden state (i.e. a vector) and a context vector at timestep t; g is a transformation function mapping into a vector with dimension of the target vocabulary size; and [i] denotes the i th component of a vector. 3 Furthermore, h t = f (h t−1 , y t−1 , c t ) is defined by an activation function, i.e. a Gated Recurrent Unit (Chung et al., 2014); and the context vector c t is a dynamical source representation at timestep t, and calculated as the weighted sum of source encodings E x , i.e. c t = α t E x .\\n\\nHere the weight α t implements an attention mechanism, and α t,i is the alignment probability of y t being aligned to x i . α t is derived through a feedforward neural network a as follows:\\nα t = a(y t−1 , h t−1 , E x )(2)\\nwhere a consists of two layers, the top one being a softmax layer. We skip the detailed definitions of a together with E x , f and g, and refer the readers to (Bahdanau et al., 2015) instead. 4 Figure 1(a) shows one slice of computational graph for NMT definition at time step t.\\n\\nTo train NMT, the following negative log-likelyhood is minimized:\\n− i log p(y i | x i ; θ)(3)\\nwhere x i , y i is a bilingual sentence pair from a given training corpus, p(y i | x i ; θ) is as defined in Eq.\\n\\n(1). Note that even though the training is conducted in a supervised manner with respect to translation, i.e., y are observable in Figure 1(a), the attention is learned in a unsupervised manner, since α is hidden. In Eq.\\n\\n(2), α t is defined only on y t−1 , h t−1 and E x but not on the target word y t , as y t is unknown at the current timestep t − 1 during the testing. Therefore, at timestep t − 1, NMT firstly tries to calculate α t , through which NMT figures out those source words will be translated next, even though the next target word y t is unavailable. From this point of view, the attention mechanism plays a role in reordering and thus can be considered as a reordering model. Unlike this attention model, conventional alignment models define the alignment α directly over x and y as follows:\\np(α | x, y) = exp(F (x, y, α)) α exp(F (x, y, α ))\\nwhere F denotes a feature function over a pair of sentences x and y together with their word alignment α, and it is either a log-probability log p(y, α | x) for a generative model like IBM models (Brown et al., 1993) or a well-designed feature function for discriminative models (Liu and Sun, 2015). In order to infer α t , alignment models can readily use the entire y, of course including y t as well, thereby they can model the alignment between x and y more sufficiently. As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models. This may be a sign that the potential of attention-based NMT is limited in end-to-end translation.\\n\\n\\nSupervised Attention\\n\\nIn this section, we introduce supervised attention to improve the alignment, which may lead to better translation performance for NMT. 5 Our basic idea is simple: similar to conventional SMT, it firstly uses a conventional aligner to obtain the alignment on the training corpus; then it employs these alignment results as supervision to train the NMT. During testing, decoding proceeds in exactly the same manner as standard NMT, since there is no alignment supervision available for unseen test sentences.\\n\\n\\nPreprocessing Alignment Supervision\\n\\nAs described in §2, the attention model outputs a soft alignment α, such that α t is a normalized probability distribution. In contrast, most aligners are typically oriented to grammar induction for conventional SMT, and they usually output 'hard' alignments, such as (Och and Ney, 2000). They only indicate whether a target word is aligned to a source word or not, and this might not correspond to a distribution for each target word. For example, one target word may align to multiple source words, or no source words at all. Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens 'eol' added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted asα.\\n\\n\\nJointly Supervising Translation and Attention\\n\\nWe propose a soft constraint method to jointly supervise the translation and attention as follows:\\n− i log p(y i | x i ; θ) + λ × ∆(α i ,α i ; θ)(4)\\nwhere α i is as defined in Eq.\\n\\n(1), ∆ is a loss function that penalizes the disagreement between α i and α i , and λ > 0 is a hyper-parameter that balances the preference between likelihood and disagreement.\\n\\nIn this way, we treat the attention variable α as an observable variable as shown in Figure 1(b), and this is different from the standard NMT as shown in Figure 1(a) in essence. Note that this training objective resembles to that in multi-task learning (Evgeniou and Pontil, 2004). Our supervised attention method has two further advantages: firstly, it is able to alleviate overfitting by means of the λ; and secondly it is easier to address the vanishing gradient problem by adding supervision into the intermediate layers of the entire network (Szegedy et al., 2015), because the supervision of α is more close to E x than y as in Figure 1(b).\\n\\nIn order to quantify the disagreement between α i andα i , three different methods are investigated in our experiments:\\n• Mean Squared Error (MSE) ∆(α i ,α i ; θ) = m n 1 2 α(θ) i m,n −α i m,n 2\\nMSE is widely used as a loss for regression tasks (Lehmann and Casella, 1998), and it directly encourages α(θ) i m,n to be equal toα i m,n .\\n\\n• Multiplication (MUL)\\n∆(α i ,α i ; θ) = − log m n α(θ) i m,n ×α i m,n\\nMUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006;Cheng et al., 2016). Note that different from those in (Cheng et al., 2016),α is not a parametrized variable but a constant in this paper.\\n\\n• Cross Entropy (CE)\\n∆(α i ,α i ; θ) = − m nα i m,n × log α(θ) i m,n\\nSince for each t, α(θ) t is a distribution, it is natural to use CE as the metric to evaluate the disagreement (Rubinstein and Kroese, 2004).\\n\\n\\nExperiments\\n\\nWe conducted experiments on two Chinese-to-English translation tasks: one is the NIST task oriented to NEWS domain, which is a large scale task and suitable to NMT; and the other is the speech translation oriented to travel domain, which is a low resource task and thus is very challenging for NMT. We used the case-insensitive BLEU4 to evaluate translation quality and adopted the multi-bleu.perl as its implementation. We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences). We compared the proposed approach with three strong baselines:\\n\\n• Moses: a phrase-based machine translation system (Koehn et al., 2007);\\n\\n• NMT1: an attention based NMT (Bahdanau et al., 2015) system at https://github.com/lisagroundhog/GroundHog;\\n\\n• NMT2: another implementation of (Bahdanau et al., 2015) at https://github.com/nyu-dl/dl4mttutorial.\\n\\nWe developed the proposed approach based on NMT2, and denoted it as SA-NMT. We followed the standard pipeline to run Moses. GIZA++ with grow-diag-final-and was used to build the translation model. We trained a 5-gram target language model on the Gigaword corpus, and used a lexicalized distortion model. All experiments were run with the default settings.\\n\\nTo train NMT1, NMT2 and SA-NMT, we employed the same settings for fair comparison. Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in (Bahdanau et al., 2015) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, 6 the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by (Zeiler, 2012). Particularly for SA-NMT, we employed a conventional word aligner to obtain the word alignment on the training data before training SA-NMT. In this paper, we used two different aligners, which are fast align and GIZA++. We tuned the hyper-parameter λ to be 0.3 on the development set, to balance the preference between the translation and alignment. Training was conducted on a single Tesla K40 GPU machine. Each update took about 3.0 seconds for both NMT2 and SA-NMT, and 2.4 seconds for NMT1. Roughly, it took about 10 days to NMT2 to finish 300000 updates.\\n\\n\\nSettings on External Alignments\\n\\nWe implemented three different losses to supervise the attention as described in §3.2. To explore their behaviors on the development set, we employed the GIZA++ to generate the alignment on the training set prior to the training SA-NMT. In Table 1, we can see that MUL is better than MSE. Furthermore, CE performs best among all losses, and thus we adopt it for the following experiments.   Table 3: BLEU comparison for large scale translation task. The development set is nist02, and the test sets are nist05,nist06 and nist08. '*' denotes that SA-NMT is significantly better than Moses, NMT1 and NMT2 with p < 0.01. Note that Moses is trained with more bilingual sentences and an additional monolingual corpus.\\n\\nIn addition, we also run fast align to generate alignments as the supervision for SA-NMT and the results were reported in Table 2. We can see that GIZA++ performs slightly better than fast align and thus we fix the external aligner as GIZA++ in the following experiments. Figure 2 shows the learning curves of NMT2 and SA-NMT on the development set. We can see that NMT2 generally obtains higher BLEU as the increasing of updates before peaking at update of 150000, while it is unstable from then on. On the other hand, SA-NMT delivers much better BLEU for the beginning updates and performs more steadily along with the updates, although it takes more updates to reach the peaking point. Table 3 reports the main end-to-end translation results for the large scale task. We find that both standard NMT generally outperforms Moses except NMT1 on nist05. The proposed SA-NMT achieves significant and consistent improvements over all three baseline systems, and it obtains the averaged gains of 2.2 BLEU points on test sets over its direct baseline NMT2. It is clear from these results that our supervised attention mechanism is highly effective in practice.\\n\\n\\nResults on Large Scale Translation Task\\n\\n\\nResults and Analysis on Alignment\\n\\nAs explained in §2, standard NMT can not use the target word information to predict its aligned source words, and thus might fail to predict the correct source words for some target words. For example, for the sentence in the training set in Figure 3 (a), NMT2 aligned 'following' to '皮诺契特 (gloss: pinochet)' rather than '继 (gloss: follow)', and worse still it aligned the word '.' to '在 (gloss: in)' rather than '。' even though this word is relatively easy to align correctly. In contrast, with the help of information from the target word itself, GIZA++ successfully aligned both 'following' and '.' to the expected source words (see Figure3(c)). With the alignment results from GIZA++ as supervision, we can see that our SA-NMT can imitate GIZA++ and thus align both words correctly. More importantly, for sentences in the unseen test set, like GIZA++, SA-NMT confidently aligned 'but' and '.' to their correct source words respectively as in Figure3(b), where NMT2 failed. It seems that SA-NMT can learn its alignment behavior from GIZA++, and subsequently apply the alignment abilities it has learned to unseen test sentences. , and (c) GIZA++ on two Chinese-English sentence pairs. The soft alignments of (c) is converted from hard alignment as in §3.1. The first row shows the alignments of the sentence pair from the training set while the second row shows the alignments from test sets.\\n\\nMethods AER GIZA++ 30.6 * NMT2 50.6 SA-NMT 43.3 * Table 4: Results on word alignment task for the large scale data. The evaluation metric is Alignment Error Rate (AER). '*' denotes that the corresponding result is significanly better than NMT2 with p < 0.01. Table 4 shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in (Liu and Sun, 2015) as the test set. Following , we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. From Table 4, we can see clearly that standard NMT (NMT2) is far behind GIZA++ in alignment quality. This shows that it is possible and promising to supervise the attention with GIZA++. With the help from GIZA++, our supervised attention based NMT (SA-NMT) significantly reduces the AER, compared with the unsupervised counterpart (NMT2). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance.\\n\\nNote that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013;Tamura et al., 2014).  Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. '*' denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p < 0.01.\\n\\n\\nResults on the Low Resource Translation Task\\n\\nFor the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out sets, respectively. We trained a 4-gram language model on the target side of training corpus for running Moses. For training all NMT systems, we employed the same settings as those in the large scale task, except that vocabulary size is 6000, batch size is 16, and the hyper-parameter λ = 1 for SA-NMT. Table 5 reports the final results. Firstly, we can see that both standard neural machine translation systems NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains much over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially.\\n\\nWhile our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs;  revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015;Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work.\\n\\n\\nRelated Work\\n\\nMany recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs.\\n\\nIt has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al. (2014) proposed a neural network method to learn a BTG reordering model. At the word level, Bisazza and Federico (2016) surveyed many word reordering models learned from alignment models for SMT, and there are some neural network based reordering models, such as (Zhang et al., 2016). Our work is inspired by these works in spirit, and it can be considered to be a recurrent neural network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs.\\n\\nSupervising the attention variables for attention-based neural networks is pioneered by . On image caption task,  supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering.\\n\\n\\nConclusion\\n\\nIt has been shown that attention mechanism in NMT is worse than conventional word alignment models in its alignment accuracy. This paper firstly provides an explanation for this by viewing the attention mechanism from the point view of reordering. Then it proposes a supervised attention for NMT with guidance from external conventional alignment models, inspired by the supervised reordering models in conventional SMT. Experiments on two Chinese-to-English translation tasks show that the proposed approach achieves better alignment results leading to significant gains relative to standard attention based NMT.\\n\\nFigure 1 :\\n1One slice of the computational graphs for both (a) NMT and (b) SA-NMT. Circles denote the hidden variables; while squares denote the observable variables, which receive supervision during training. The difference (marked in red) in (b) regarding to (a) is treating α t as an observable variable instead of a hidden variable.\\n\\nFigure 2 :\\n2Learning curves of NMT2 and SA-NMT on the development set.\\n\\nFigure 3 :\\n3Example (soft) alignments of (a) NMT2 (i.e., standard NMT with unsupervised attention), (b) SA-NMT (i.e. NMT with supervised attention)\\n\\n\\nTable 1: Performance of SA-NMT on development set for different loss functions to supervise the attention in terms of BLEU.Alignment Losses \\n\\nBLEU \\nMean Squared Error (MSE) \\n39.4 \\nMultiplication (MUL) \\n39.6 \\nCross Entropy (CE) \\n40.0 \\n\\nAlignment Methods BLEU \\nfast align \\n39.6 \\nGIZA++ \\n40.0 \\n\\nTable 2: Comparision of aligners between fast align and GIZA++ for SA-NMT in terms of BLEU on the \\ndevelopment set. \\n\\n4.1 The Large Scale Translation Task \\n\\n4.1.1 Preparation \\n\\n\\nIn that sense, yt in Eq.(1) also denotes the index of this word in its vocabulary.\\nIn the original paper, αt is not explicitly dependent on the yt−1 in Eq.(2), but this dependency was explicitly retained in our direct baseline NMT2.5  Although the alignment is loosely related to the downstream translation(Liu and Sun, 2015), substantial improvements in alignment usually leads to the improvements in translation as observed in our experiments.\\nThis excludes all the sentences longer than 50 words in either source or target side only for NMT systems, but for Moses we use the entire training data.\\nAcknowledgementsWe would like to thank Xugang Lu for invaluable discussions and three anonymous reviewers for many valuable comments and helpful suggestions on this work.\\nIncorporating discrete translation lexicons into neural machine translation. Philip Arthur, Graham Neubig, Satoshi Nakamura, abs/1606.02006CoRRPhilip Arthur, Graham Neubig, and Satoshi Nakamura. 2016. Incorporating discrete translation lexicons into neural machine translation. CoRR, abs/1606.02006.\\n\\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, abs/1409.0473CoRRDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.\\n\\nA survey of word reordering in statistical machine translation: Computational models and language phenomena. Arianna Bisazza, Marcello Federico, Computational Linguistics. 42Arianna Bisazza and Marcello Federico. 2016. A survey of word reordering in statistical machine translation: Computational models and language phenomena. Computational Linguistics, 42.\\n\\nThe mathematics of statistical machine translation: Parameter estimation. F Peter, Brown, J Della Vincent, Stephen A Pietra, Robert L Della Pietra, Mercer, Comput. Linguist. 192Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Comput. Linguist., 19(2):263-311.\\n\\nGuided alignment training for topic-aware neural machine translation. Wenhu Chen, Evgeny Matusov, Shahram Khadivi, Jan-Thorsten Peter, Proceedings of AMTA. AMTAWenhu Chen, Evgeny Matusov, Shahram Khadivi, and Jan-Thorsten Peter. 2016. Guided alignment training for topic-aware neural machine translation. In Proceedings of AMTA.\\n\\nAgreement-based joint training for bidirectional attention-based neural machine translation. Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu, Proceedings of IJCAI. IJCAIYong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Agreement-based joint training for bidirectional attention-based neural machine translation. In Proceedings of IJCAI.\\n\\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Kyunghyun Aglar Gülçehre, Yoshua Cho, Bengio, abs/1412.3555CoRRJunyoung Chung, Ç aglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555.\\n\\nIncorporating structural alignment biases into an attentional neural translation model. Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, Gholamreza Haffari, Proceedings of NAACL-HLT. NAACL-HLTTrevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, and Gholamreza Haffari. 2016. Incorporating structural alignment biases into an attentional neural translation model. In Proceedings of NAACL-HLT.\\n\\nFast and robust neural network joint models for statistical machine translation. Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, John Makhoul, Proceedings of ACL. ACLJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of ACL.\\n\\nA simple, fast, and effective reparameterization of ibm model 2. Chris Dyer, Victor Chahuneau, Noah A Smith, Proc. NAACL. NAACLChris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In In Proc. NAACL.\\n\\nRegularized multi-task learning. Theodoros Evgeniou, Massimiliano Pontil, Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining4Theodoros Evgeniou and Massimiliano Pontil. 2004. Regularized multi-task learning. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '04.\\n\\nImplicit distortion and fertility models for attention-based encoder-decoder NMT model. Shujie Shi Feng, Mu Liu, Ming Li, Zhou, abs/1601.03317CoRRShi Feng, Shujie Liu, Mu Li, and Ming Zhou. 2016. Implicit distortion and fertility models for attention-based encoder-decoder NMT model. CoRR, abs/1601.03317.\\n\\nMoses: open source toolkit for statistical machine translation. P Koehn, H Hoang, A Birch, C Callison-Burch, M Federico, N Bertoldi, B Cowan, W Shen, C Moran, R Zens, C Dyer, O Bojar, A Constantin, E Herbst, Proceedings of ACL: Demonstrations. ACL: DemonstrationsP. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of ACL: Demonstrations.\\n\\nTheory of Point Estimation. E L Lehmann, G Casella, Springer VerlagE.L. Lehmann and G. Casella. 1998. Theory of Point Estimation. Springer Verlag.\\n\\nA neural reordering model for phrasebased translation. Peng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, Dakun Zhang, Proceedings of COLING. COLINGPeng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, and Dakun Zhang. 2014. A neural reordering model for phrase- based translation. In Proceedings of COLING.\\n\\nAlignment by agreement. Percy Liang, Ben Taskar, Dan Klein, Proceedings of HLT-NAACL. HLT-NAACLPercy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL.\\n\\nContrastive unsupervised word alignment with non-local features. Yang Liu, Maosong Sun, Yang Liu and Maosong Sun. 2015. Contrastive unsupervised word alignment with non-local features.\\n\\nAttention correctness in neural image captioning. Chenxi Liu, Junhua Mao, Fei Sha, Alan L Yuille, abs/1605.09553CoRRChenxi Liu, Junhua Mao, Fei Sha, and Alan L. Yuille. 2016. Attention correctness in neural image captioning. CoRR, abs/1605.09553.\\n\\nStanford neural machine translation systems for spoken language domains. Minh-Thang Luong, Christopher D Manning, Proceedings of IWSLT. IWSLTMinh-Thang Luong and Christopher D. Manning. 2015. Stanford neural machine translation systems for spoken language domains. In Proceedings of IWSLT.\\n\\nEffective approaches to attention-based neural machine translation. Thang Luong, Hieu Pham, Christopher D Manning, Proceedings of EMNLP. EMNLPThang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of EMNLP.\\n\\nSupervised attentions for neural machine translation. Haitao Mi, Zhiguo Wang, Abe Ittycheriah, Proceedings of EMNLP. EMNLPHaitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016. Supervised attentions for neural machine translation. In Proceedings of EMNLP.\\n\\nNeural reranking improves subjective quality of machine translation: NAIST at WAT2015. Graham Neubig, Makoto Morishita, Satoshi Nakamura, Proceedings of the 2nd Workshop on Asian Translation (WAT2015). the 2nd Workshop on Asian Translation (WAT2015)Graham Neubig, Makoto Morishita, and Satoshi Nakamura. 2015. Neural reranking improves subjective qual- ity of machine translation: NAIST at WAT2015. In Proceedings of the 2nd Workshop on Asian Translation (WAT2015).\\n\\nImproved statistical alignment models. Josef Franz, Hermann Och, Ney, Proceedings of ACL. ACLFranz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL, pages 440-447.\\n\\nThe Cross Entropy Method: A Unified Approach To Combinatorial Optimization, Monte-carlo Simulation (Information Science and Statistics). Reuven Y Rubinstein, Dirk P Kroese, Springer-VerlagNew York, Inc., Secaucus, NJ, USAReuven Y. Rubinstein and Dirk P. Kroese. 2004. The Cross Entropy Method: A Unified Approach To Combina- torial Optimization, Monte-carlo Simulation (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA.\\n\\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, V Quoc, Le, Proceedings of NIPS. NIPSIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2015. Sequence to sequence learning with neural networks. In Proceedings of NIPS.\\n\\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of Computer Vision and Pattern Recognition (CVPR).\\n\\nRecurrent neural networks for word alignment model. Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Proceedings of ACL. ACLAkihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2014. Recurrent neural networks for word alignment model. In Proceedings of ACL.\\n\\nModeling coverage for neural machine translation. Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li, Proceedings of ACL. ACLZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine translation. In Proceedings of ACL.\\n\\nMaximum entropy based phrase reordering model for statistical machine translation. Deyi Xiong, Qun Liu, Shouxun Lin, Proceedings of ACL. ACLDeyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of ACL.\\n\\nWord alignment modeling with context dependent deep neural network. Nan Yang, Shujie Liu, Mu Li, Ming Zhou, Nenghai Yu, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsLong Papers1Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word alignment modeling with context de- pendent deep neural network. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), August.\\n\\nADADELTA: an adaptive learning rate method. D Matthew, Zeiler, CoRRMatthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR.\\n\\nLearning local word reorderings for hierarchical phrase-based statistical machine translation. Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Hai Zhao, Graham Neubig, Satoshi Nakamura, Machine TranslationJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Hai Zhao, Graham Neubig, and Satoshi Nakamura. 2016. Learn- ing local word reorderings for hierarchical phrase-based statistical machine translation. Machine Translation.\\n\",\n"," 'annotations': {'abstract': '[{\"end\":1918,\"start\":913}]',\n","  'author': '[{\"end\":221,\"start\":75},{\"end\":355,\"start\":222},{\"end\":488,\"start\":356},{\"end\":624,\"start\":489}]',\n","  'authoraffiliation': '[{\"end\":220,\"start\":103},{\"end\":354,\"start\":237},{\"end\":487,\"start\":370},{\"end\":623,\"start\":506}]',\n","  'authorfirstname': '[{\"end\":80,\"start\":75},{\"end\":227,\"start\":222},{\"end\":362,\"start\":356},{\"end\":497,\"start\":489}]',\n","  'authorlastname': '[{\"end\":84,\"start\":81},{\"end\":235,\"start\":228},{\"end\":368,\"start\":363},{\"end\":504,\"start\":498}]',\n","  'bibauthor': '[{\"end\":28307,\"start\":28292},{\"end\":28322,\"start\":28307},{\"end\":28340,\"start\":28322},{\"end\":28605,\"start\":28587},{\"end\":28620,\"start\":28605},{\"end\":28635,\"start\":28620},{\"end\":28929,\"start\":28912},{\"end\":28948,\"start\":28929},{\"end\":29246,\"start\":29237},{\"end\":29253,\"start\":29246},{\"end\":29270,\"start\":29253},{\"end\":29288,\"start\":29270},{\"end\":29311,\"start\":29288},{\"end\":29319,\"start\":29311},{\"end\":29625,\"start\":29613},{\"end\":29641,\"start\":29625},{\"end\":29658,\"start\":29641},{\"end\":29678,\"start\":29658},{\"end\":29978,\"start\":29966},{\"end\":29990,\"start\":29978},{\"end\":30003,\"start\":29990},{\"end\":30011,\"start\":30003},{\"end\":30019,\"start\":30011},{\"end\":30032,\"start\":30019},{\"end\":30042,\"start\":30032},{\"end\":30368,\"start\":30352},{\"end\":30394,\"start\":30368},{\"end\":30406,\"start\":30394},{\"end\":30414,\"start\":30406},{\"end\":30706,\"start\":30693},{\"end\":30725,\"start\":30706},{\"end\":30745,\"start\":30725},{\"end\":30759,\"start\":30745},{\"end\":30771,\"start\":30759},{\"end\":30791,\"start\":30771},{\"end\":31147,\"start\":31133},{\"end\":31159,\"start\":31147},{\"end\":31177,\"start\":31159},{\"end\":31191,\"start\":31177},{\"end\":31209,\"start\":31191},{\"end\":31223,\"start\":31209},{\"end\":31528,\"start\":31516},{\"end\":31546,\"start\":31528},{\"end\":31560,\"start\":31546},{\"end\":31771,\"start\":31751},{\"end\":31792,\"start\":31771},{\"end\":32280,\"start\":32263},{\"end\":32288,\"start\":32280},{\"end\":32297,\"start\":32288},{\"end\":32303,\"start\":32297},{\"end\":32555,\"start\":32546},{\"end\":32564,\"start\":32555},{\"end\":32573,\"start\":32564},{\"end\":32591,\"start\":32573},{\"end\":32603,\"start\":32591},{\"end\":32615,\"start\":32603},{\"end\":32624,\"start\":32615},{\"end\":32632,\"start\":32624},{\"end\":32641,\"start\":32632},{\"end\":32649,\"start\":32641},{\"end\":32657,\"start\":32649},{\"end\":32666,\"start\":32657},{\"end\":32680,\"start\":32666},{\"end\":32690,\"start\":32680},{\"end\":33058,\"start\":33045},{\"end\":33069,\"start\":33058},{\"end\":33229,\"start\":33220},{\"end\":33239,\"start\":33229},{\"end\":33252,\"start\":33239},{\"end\":33267,\"start\":33252},{\"end\":33280,\"start\":33267},{\"end\":33500,\"start\":33487},{\"end\":33512,\"start\":33500},{\"end\":33523,\"start\":33512},{\"end\":33733,\"start\":33723},{\"end\":33746,\"start\":33733},{\"end\":33906,\"start\":33894},{\"end\":33918,\"start\":33906},{\"end\":33927,\"start\":33918},{\"end\":33942,\"start\":33927},{\"end\":34183,\"start\":34165},{\"end\":34206,\"start\":34183},{\"end\":34464,\"start\":34451},{\"end\":34475,\"start\":34464},{\"end\":34498,\"start\":34475},{\"end\":34742,\"start\":34731},{\"end\":34755,\"start\":34742},{\"end\":34772,\"start\":34755},{\"end\":35032,\"start\":35017},{\"end\":35050,\"start\":35032},{\"end\":35068,\"start\":35050},{\"end\":35449,\"start\":35436},{\"end\":35462,\"start\":35449},{\"end\":35467,\"start\":35462},{\"end\":35765,\"start\":35744},{\"end\":35780,\"start\":35765},{\"end\":36134,\"start\":36118},{\"end\":36149,\"start\":36134},{\"end\":36157,\"start\":36149},{\"end\":36161,\"start\":36157},{\"end\":36367,\"start\":36348},{\"end\":36376,\"start\":36367},{\"end\":36390,\"start\":36376},{\"end\":36407,\"start\":36390},{\"end\":36419,\"start\":36407},{\"end\":36438,\"start\":36419},{\"end\":36453,\"start\":36438},{\"end\":36472,\"start\":36453},{\"end\":36491,\"start\":36472},{\"end\":36920,\"start\":36904},{\"end\":36935,\"start\":36920},{\"end\":36952,\"start\":36935},{\"end\":37172,\"start\":37159},{\"end\":37186,\"start\":37172},{\"end\":37196,\"start\":37186},{\"end\":37209,\"start\":37196},{\"end\":37218,\"start\":37209},{\"end\":37479,\"start\":37467},{\"end\":37488,\"start\":37479},{\"end\":37501,\"start\":37488},{\"end\":37753,\"start\":37743},{\"end\":37765,\"start\":37753},{\"end\":37772,\"start\":37765},{\"end\":37783,\"start\":37772},{\"end\":37795,\"start\":37783},{\"end\":38280,\"start\":38269},{\"end\":38288,\"start\":38280},{\"end\":38477,\"start\":38463},{\"end\":38492,\"start\":38477},{\"end\":38509,\"start\":38492},{\"end\":38519,\"start\":38509},{\"end\":38534,\"start\":38519},{\"end\":38552,\"start\":38534}]',\n","  'bibauthorfirstname': '[{\"end\":28298,\"start\":28292},{\"end\":28313,\"start\":28307},{\"end\":28329,\"start\":28322},{\"end\":28594,\"start\":28587},{\"end\":28614,\"start\":28605},{\"end\":28626,\"start\":28620},{\"end\":28919,\"start\":28912},{\"end\":28937,\"start\":28929},{\"end\":29238,\"start\":29237},{\"end\":29254,\"start\":29253},{\"end\":29260,\"start\":29255},{\"end\":29277,\"start\":29270},{\"end\":29279,\"start\":29278},{\"end\":29294,\"start\":29288},{\"end\":29296,\"start\":29295},{\"end\":29618,\"start\":29613},{\"end\":29631,\"start\":29625},{\"end\":29648,\"start\":29641},{\"end\":29670,\"start\":29658},{\"end\":29970,\"start\":29966},{\"end\":29983,\"start\":29978},{\"end\":29998,\"start\":29990},{\"end\":30006,\"start\":30003},{\"end\":30014,\"start\":30011},{\"end\":30026,\"start\":30019},{\"end\":30036,\"start\":30032},{\"end\":30360,\"start\":30352},{\"end\":30377,\"start\":30368},{\"end\":30400,\"start\":30394},{\"end\":30699,\"start\":30693},{\"end\":30717,\"start\":30706},{\"end\":30734,\"start\":30725},{\"end\":30753,\"start\":30745},{\"end\":30764,\"start\":30759},{\"end\":30781,\"start\":30771},{\"end\":31138,\"start\":31133},{\"end\":31152,\"start\":31147},{\"end\":31169,\"start\":31159},{\"end\":31183,\"start\":31177},{\"end\":31198,\"start\":31191},{\"end\":31213,\"start\":31209},{\"end\":31521,\"start\":31516},{\"end\":31534,\"start\":31528},{\"end\":31550,\"start\":31546},{\"end\":31552,\"start\":31551},{\"end\":31760,\"start\":31751},{\"end\":31783,\"start\":31771},{\"end\":32269,\"start\":32263},{\"end\":32282,\"start\":32280},{\"end\":32292,\"start\":32288},{\"end\":32547,\"start\":32546},{\"end\":32556,\"start\":32555},{\"end\":32565,\"start\":32564},{\"end\":32574,\"start\":32573},{\"end\":32592,\"start\":32591},{\"end\":32604,\"start\":32603},{\"end\":32616,\"start\":32615},{\"end\":32625,\"start\":32624},{\"end\":32633,\"start\":32632},{\"end\":32642,\"start\":32641},{\"end\":32650,\"start\":32649},{\"end\":32658,\"start\":32657},{\"end\":32667,\"start\":32666},{\"end\":32681,\"start\":32680},{\"end\":33046,\"start\":33045},{\"end\":33048,\"start\":33047},{\"end\":33059,\"start\":33058},{\"end\":33224,\"start\":33220},{\"end\":33233,\"start\":33229},{\"end\":33246,\"start\":33239},{\"end\":33259,\"start\":33252},{\"end\":33272,\"start\":33267},{\"end\":33492,\"start\":33487},{\"end\":33503,\"start\":33500},{\"end\":33515,\"start\":33512},{\"end\":33727,\"start\":33723},{\"end\":33740,\"start\":33733},{\"end\":33900,\"start\":33894},{\"end\":33912,\"start\":33906},{\"end\":33921,\"start\":33918},{\"end\":33931,\"start\":33927},{\"end\":33933,\"start\":33932},{\"end\":34175,\"start\":34165},{\"end\":34194,\"start\":34183},{\"end\":34196,\"start\":34195},{\"end\":34456,\"start\":34451},{\"end\":34468,\"start\":34464},{\"end\":34486,\"start\":34475},{\"end\":34488,\"start\":34487},{\"end\":34737,\"start\":34731},{\"end\":34748,\"start\":34742},{\"end\":34758,\"start\":34755},{\"end\":35023,\"start\":35017},{\"end\":35038,\"start\":35032},{\"end\":35057,\"start\":35050},{\"end\":35441,\"start\":35436},{\"end\":35456,\"start\":35449},{\"end\":35750,\"start\":35744},{\"end\":35752,\"start\":35751},{\"end\":35769,\"start\":35765},{\"end\":35771,\"start\":35770},{\"end\":36122,\"start\":36118},{\"end\":36139,\"start\":36134},{\"end\":36150,\"start\":36149},{\"end\":36357,\"start\":36348},{\"end\":36370,\"start\":36367},{\"end\":36384,\"start\":36376},{\"end\":36396,\"start\":36390},{\"end\":36412,\"start\":36407},{\"end\":36427,\"start\":36419},{\"end\":36445,\"start\":36438},{\"end\":36460,\"start\":36453},{\"end\":36478,\"start\":36472},{\"end\":36911,\"start\":36904},{\"end\":36924,\"start\":36920},{\"end\":36943,\"start\":36935},{\"end\":37167,\"start\":37159},{\"end\":37181,\"start\":37172},{\"end\":37190,\"start\":37186},{\"end\":37203,\"start\":37196},{\"end\":37213,\"start\":37209},{\"end\":37471,\"start\":37467},{\"end\":37482,\"start\":37479},{\"end\":37495,\"start\":37488},{\"end\":37746,\"start\":37743},{\"end\":37759,\"start\":37753},{\"end\":37767,\"start\":37765},{\"end\":37776,\"start\":37772},{\"end\":37790,\"start\":37783},{\"end\":38270,\"start\":38269},{\"end\":38469,\"start\":38463},{\"end\":38482,\"start\":38477},{\"end\":38500,\"start\":38492},{\"end\":38512,\"start\":38509},{\"end\":38525,\"start\":38519},{\"end\":38541,\"start\":38534}]',\n","  'bibauthorlastname': '[{\"end\":28305,\"start\":28299},{\"end\":28320,\"start\":28314},{\"end\":28338,\"start\":28330},{\"end\":28603,\"start\":28595},{\"end\":28618,\"start\":28615},{\"end\":28633,\"start\":28627},{\"end\":28927,\"start\":28920},{\"end\":28946,\"start\":28938},{\"end\":29244,\"start\":29239},{\"end\":29251,\"start\":29246},{\"end\":29268,\"start\":29261},{\"end\":29286,\"start\":29280},{\"end\":29309,\"start\":29297},{\"end\":29317,\"start\":29311},{\"end\":29623,\"start\":29619},{\"end\":29639,\"start\":29632},{\"end\":29656,\"start\":29649},{\"end\":29676,\"start\":29671},{\"end\":29976,\"start\":29971},{\"end\":29988,\"start\":29984},{\"end\":30001,\"start\":29999},{\"end\":30009,\"start\":30007},{\"end\":30017,\"start\":30015},{\"end\":30030,\"start\":30027},{\"end\":30040,\"start\":30037},{\"end\":30366,\"start\":30361},{\"end\":30392,\"start\":30378},{\"end\":30404,\"start\":30401},{\"end\":30412,\"start\":30406},{\"end\":30704,\"start\":30700},{\"end\":30723,\"start\":30718},{\"end\":30743,\"start\":30735},{\"end\":30757,\"start\":30754},{\"end\":30769,\"start\":30765},{\"end\":30789,\"start\":30782},{\"end\":31145,\"start\":31139},{\"end\":31157,\"start\":31153},{\"end\":31175,\"start\":31170},{\"end\":31189,\"start\":31184},{\"end\":31207,\"start\":31199},{\"end\":31221,\"start\":31214},{\"end\":31526,\"start\":31522},{\"end\":31544,\"start\":31535},{\"end\":31558,\"start\":31553},{\"end\":31769,\"start\":31761},{\"end\":31790,\"start\":31784},{\"end\":32278,\"start\":32270},{\"end\":32286,\"start\":32283},{\"end\":32295,\"start\":32293},{\"end\":32301,\"start\":32297},{\"end\":32553,\"start\":32548},{\"end\":32562,\"start\":32557},{\"end\":32571,\"start\":32566},{\"end\":32589,\"start\":32575},{\"end\":32601,\"start\":32593},{\"end\":32613,\"start\":32605},{\"end\":32622,\"start\":32617},{\"end\":32630,\"start\":32626},{\"end\":32639,\"start\":32634},{\"end\":32647,\"start\":32643},{\"end\":32655,\"start\":32651},{\"end\":32664,\"start\":32659},{\"end\":32678,\"start\":32668},{\"end\":32688,\"start\":32682},{\"end\":33056,\"start\":33049},{\"end\":33067,\"start\":33060},{\"end\":33227,\"start\":33225},{\"end\":33237,\"start\":33234},{\"end\":33250,\"start\":33247},{\"end\":33265,\"start\":33260},{\"end\":33278,\"start\":33273},{\"end\":33498,\"start\":33493},{\"end\":33510,\"start\":33504},{\"end\":33521,\"start\":33516},{\"end\":33731,\"start\":33728},{\"end\":33744,\"start\":33741},{\"end\":33904,\"start\":33901},{\"end\":33916,\"start\":33913},{\"end\":33925,\"start\":33922},{\"end\":33940,\"start\":33934},{\"end\":34181,\"start\":34176},{\"end\":34204,\"start\":34197},{\"end\":34462,\"start\":34457},{\"end\":34473,\"start\":34469},{\"end\":34496,\"start\":34489},{\"end\":34740,\"start\":34738},{\"end\":34753,\"start\":34749},{\"end\":34770,\"start\":34759},{\"end\":35030,\"start\":35024},{\"end\":35048,\"start\":35039},{\"end\":35066,\"start\":35058},{\"end\":35447,\"start\":35442},{\"end\":35460,\"start\":35457},{\"end\":35465,\"start\":35462},{\"end\":35763,\"start\":35753},{\"end\":35778,\"start\":35772},{\"end\":36132,\"start\":36123},{\"end\":36147,\"start\":36140},{\"end\":36155,\"start\":36151},{\"end\":36159,\"start\":36157},{\"end\":36365,\"start\":36358},{\"end\":36374,\"start\":36371},{\"end\":36388,\"start\":36385},{\"end\":36405,\"start\":36397},{\"end\":36417,\"start\":36413},{\"end\":36436,\"start\":36428},{\"end\":36451,\"start\":36446},{\"end\":36470,\"start\":36461},{\"end\":36489,\"start\":36479},{\"end\":36918,\"start\":36912},{\"end\":36933,\"start\":36925},{\"end\":36950,\"start\":36944},{\"end\":37170,\"start\":37168},{\"end\":37184,\"start\":37182},{\"end\":37194,\"start\":37191},{\"end\":37207,\"start\":37204},{\"end\":37216,\"start\":37214},{\"end\":37477,\"start\":37472},{\"end\":37486,\"start\":37483},{\"end\":37499,\"start\":37496},{\"end\":37751,\"start\":37747},{\"end\":37763,\"start\":37760},{\"end\":37770,\"start\":37768},{\"end\":37781,\"start\":37777},{\"end\":37793,\"start\":37791},{\"end\":38278,\"start\":38271},{\"end\":38286,\"start\":38280},{\"end\":38475,\"start\":38470},{\"end\":38490,\"start\":38483},{\"end\":38507,\"start\":38501},{\"end\":38517,\"start\":38513},{\"end\":38532,\"start\":38526},{\"end\":38550,\"start\":38542}]',\n","  'bibentry': '[{\"attributes\":{\"doi\":\"abs/1606.02006\",\"id\":\"b0\"},\"end\":28514,\"start\":28215},{\"attributes\":{\"doi\":\"abs/1409.0473\",\"id\":\"b1\"},\"end\":28801,\"start\":28516},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7384097},\"end\":29161,\"start\":28803},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13259913},\"end\":29541,\"start\":29163},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17078659},\"end\":29871,\"start\":29543},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3935945},\"end\":30272,\"start\":29873},{\"attributes\":{\"doi\":\"abs/1412.3555\",\"id\":\"b6\"},\"end\":30603,\"start\":30274},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1964946},\"end\":31050,\"start\":30605},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7417943},\"end\":31449,\"start\":31052},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8476273},\"end\":31716,\"start\":31451},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":719551},\"end\":32173,\"start\":31718},{\"attributes\":{\"doi\":\"abs/1601.03317\",\"id\":\"b11\"},\"end\":32480,\"start\":32175},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":794019},\"end\":33015,\"start\":32482},{\"attributes\":{\"id\":\"b13\"},\"end\":33163,\"start\":33017},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":10598485},\"end\":33461,\"start\":33165},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":618683},\"end\":33656,\"start\":33463},{\"attributes\":{\"id\":\"b16\"},\"end\":33842,\"start\":33658},{\"attributes\":{\"doi\":\"abs/1605.09553\",\"id\":\"b17\"},\"end\":34090,\"start\":33844},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14646537},\"end\":34381,\"start\":34092},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1998416},\"end\":34675,\"start\":34383},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18193214},\"end\":34928,\"start\":34677},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":110317},\"end\":35395,\"start\":34930},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5284722},\"end\":35605,\"start\":35397},{\"attributes\":{\"id\":\"b23\"},\"end\":36064,\"start\":35607},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7961699},\"end\":36314,\"start\":36066},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206592484},\"end\":36850,\"start\":36316},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9316168},\"end\":37107,\"start\":36852},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":146843},\"end\":37382,\"start\":37109},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6832980},\"end\":37673,\"start\":37384},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":148051},\"end\":38223,\"start\":37675},{\"attributes\":{\"id\":\"b30\"},\"end\":38366,\"start\":38225},{\"attributes\":{\"id\":\"b31\"},\"end\":38787,\"start\":38368}]',\n","  'bibref': '[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2057,\"start\":2034},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2080,\"start\":2057},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2480,\"start\":2457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2843,\"start\":2820},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3209,\"start\":3190},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3227,\"start\":3209},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3245,\"start\":3227},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3478,\"start\":3458},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3908,\"start\":3888},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3927,\"start\":3908},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3954,\"start\":3927},{\"end\":4196,\"start\":4169},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4230,\"start\":4211},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5137,\"start\":5115},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5574,\"start\":5551},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7389,\"start\":7369},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7958,\"start\":7935},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9341,\"start\":9321},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9423,\"start\":9404},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9714,\"start\":9694},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10715,\"start\":10696},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11212,\"start\":11191},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12235,\"start\":12208},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12524,\"start\":12502},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12875,\"start\":12848},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13130,\"start\":13110},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13149,\"start\":13130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13205,\"start\":13185},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13479,\"start\":13450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14327,\"start\":14307},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14384,\"start\":14361},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14497,\"start\":14474},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15133,\"start\":15110},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15489,\"start\":15475},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19865,\"start\":19846},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21105,\"start\":21086},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21125,\"start\":21105},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22660,\"start\":22640},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23016,\"start\":22995},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23034,\"start\":23016},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23260,\"start\":23244},{\"end\":23418,\"start\":23395},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23530,\"start\":23511},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23629,\"start\":23611},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24132,\"start\":24113},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24209,\"start\":24190},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24296,\"start\":24280},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24409,\"start\":24382},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24573,\"start\":24553},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25250,\"start\":25234},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25273,\"start\":25255},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27769,\"start\":27750}]',\n","  'bibtitle': '[{\"end\":28910,\"start\":28803},{\"end\":29235,\"start\":29163},{\"end\":29611,\"start\":29543},{\"end\":29964,\"start\":29873},{\"end\":30691,\"start\":30605},{\"end\":31131,\"start\":31052},{\"end\":31514,\"start\":31451},{\"end\":31749,\"start\":31718},{\"end\":32544,\"start\":32482},{\"end\":33218,\"start\":33165},{\"end\":33485,\"start\":33463},{\"end\":34163,\"start\":34092},{\"end\":34449,\"start\":34383},{\"end\":34729,\"start\":34677},{\"end\":35015,\"start\":34930},{\"end\":35434,\"start\":35397},{\"end\":36116,\"start\":36066},{\"end\":36346,\"start\":36316},{\"end\":36902,\"start\":36852},{\"end\":37157,\"start\":37109},{\"end\":37465,\"start\":37384},{\"end\":37741,\"start\":37675}]',\n","  'bibvenue': '[{\"end\":29703,\"start\":29699},{\"end\":30069,\"start\":30064},{\"end\":30826,\"start\":30817},{\"end\":31246,\"start\":31243},{\"end\":31578,\"start\":31573},{\"end\":31977,\"start\":31893},{\"end\":32745,\"start\":32726},{\"end\":33309,\"start\":33303},{\"end\":33558,\"start\":33549},{\"end\":34233,\"start\":34228},{\"end\":34525,\"start\":34520},{\"end\":34799,\"start\":34794},{\"end\":35179,\"start\":35132},{\"end\":35490,\"start\":35487},{\"end\":36186,\"start\":36182},{\"end\":36600,\"start\":36554},{\"end\":36975,\"start\":36972},{\"end\":37241,\"start\":37238},{\"end\":37524,\"start\":37521},{\"end\":37956,\"start\":37884},{\"end\":28290,\"start\":28215},{\"end\":28585,\"start\":28516},{\"end\":28973,\"start\":28948},{\"end\":29335,\"start\":29319},{\"end\":29697,\"start\":29678},{\"end\":30062,\"start\":30042},{\"end\":30350,\"start\":30274},{\"end\":30815,\"start\":30791},{\"end\":31241,\"start\":31223},{\"end\":31571,\"start\":31560},{\"end\":31891,\"start\":31792},{\"end\":32261,\"start\":32175},{\"end\":32724,\"start\":32690},{\"end\":33043,\"start\":33017},{\"end\":33301,\"start\":33280},{\"end\":33547,\"start\":33523},{\"end\":33721,\"start\":33658},{\"end\":33892,\"start\":33844},{\"end\":34226,\"start\":34206},{\"end\":34518,\"start\":34498},{\"end\":34792,\"start\":34772},{\"end\":35130,\"start\":35068},{\"end\":35485,\"start\":35467},{\"end\":35742,\"start\":35607},{\"end\":36180,\"start\":36161},{\"end\":36552,\"start\":36491},{\"end\":36970,\"start\":36952},{\"end\":37236,\"start\":37218},{\"end\":37519,\"start\":37501},{\"end\":37882,\"start\":37795},{\"end\":38267,\"start\":38225},{\"end\":38461,\"start\":38368}]',\n","  'figure': '[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26750,\"start\":26413},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26822,\"start\":26751},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26971,\"start\":26823},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27443,\"start\":26972}]',\n","  'figurecaption': '[{\"end\":26750,\"start\":26426},{\"end\":26822,\"start\":26764},{\"end\":26971,\"start\":26836},{\"end\":27097,\"start\":26974}]',\n","  'figureref': '[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4961,\"start\":4953},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7978,\"start\":7970},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8404,\"start\":8396},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12048,\"start\":12040},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12117,\"start\":12109},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12597,\"start\":12589},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17079,\"start\":17071},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18284,\"start\":18276},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18990,\"start\":18980}]',\n","  'formula': '[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6911,\"start\":6790},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6999,\"start\":6911},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7775,\"start\":7743},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8150,\"start\":8123},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9124,\"start\":9074},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11744,\"start\":11695},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12797,\"start\":12723},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13010,\"start\":12963},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13338,\"start\":13291}]',\n","  'paragraph': '[{\"end\":2712,\"start\":1934},{\"end\":3955,\"start\":2714},{\"end\":5184,\"start\":3957},{\"end\":5575,\"start\":5186},{\"end\":6749,\"start\":5577},{\"end\":7551,\"start\":7000},{\"end\":7742,\"start\":7553},{\"end\":8055,\"start\":7776},{\"end\":8122,\"start\":8057},{\"end\":8263,\"start\":8151},{\"end\":8485,\"start\":8265},{\"end\":9073,\"start\":8487},{\"end\":9857,\"start\":9125},{\"end\":10388,\"start\":9882},{\"end\":11546,\"start\":10428},{\"end\":11694,\"start\":11596},{\"end\":11775,\"start\":11745},{\"end\":11953,\"start\":11777},{\"end\":12601,\"start\":11955},{\"end\":12722,\"start\":12603},{\"end\":12938,\"start\":12798},{\"end\":12962,\"start\":12940},{\"end\":13268,\"start\":13011},{\"end\":13290,\"start\":13270},{\"end\":13480,\"start\":13339},{\"end\":14254,\"start\":13496},{\"end\":14328,\"start\":14256},{\"end\":14438,\"start\":14330},{\"end\":14541,\"start\":14440},{\"end\":14898,\"start\":14543},{\"end\":16049,\"start\":14900},{\"end\":16797,\"start\":16085},{\"end\":17954,\"start\":16799},{\"end\":19429,\"start\":18034},{\"end\":20649,\"start\":19431},{\"end\":21336,\"start\":20651},{\"end\":22452,\"start\":21385},{\"end\":23117,\"start\":22454},{\"end\":23947,\"start\":23134},{\"end\":24855,\"start\":23949},{\"end\":25784,\"start\":24857},{\"end\":26412,\"start\":25799}]',\n","  'publisher': None,\n","  'sectionheader': '[{\"attributes\":{\"n\":\"1\"},\"end\":1932,\"start\":1920},{\"attributes\":{\"n\":\"2\"},\"end\":6789,\"start\":6752},{\"attributes\":{\"n\":\"3\"},\"end\":9880,\"start\":9860},{\"attributes\":{\"n\":\"3.1\"},\"end\":10426,\"start\":10391},{\"attributes\":{\"n\":\"3.2\"},\"end\":11594,\"start\":11549},{\"attributes\":{\"n\":\"4\"},\"end\":13494,\"start\":13483},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":16083,\"start\":16052},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":17996,\"start\":17957},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":18032,\"start\":17999},{\"attributes\":{\"n\":\"4.2\"},\"end\":21383,\"start\":21339},{\"attributes\":{\"n\":\"5\"},\"end\":23132,\"start\":23120},{\"attributes\":{\"n\":\"6\"},\"end\":25797,\"start\":25787},{\"end\":26424,\"start\":26414},{\"end\":26762,\"start\":26752},{\"end\":26834,\"start\":26824}]',\n","  'table': '[{\"end\":27443,\"start\":27097}]',\n","  'tableref': '[{\"end\":16332,\"start\":16325},{\"end\":16483,\"start\":16476},{\"end\":16928,\"start\":16921},{\"end\":17495,\"start\":17488},{\"end\":19488,\"start\":19481},{\"end\":19697,\"start\":19690},{\"end\":20181,\"start\":20174},{\"end\":20731,\"start\":20724},{\"end\":21135,\"start\":21128},{\"end\":21938,\"start\":21931}]',\n","  'title': '[{\"end\":53,\"start\":1},{\"end\":677,\"start\":625}]',\n","  'venue': '[{\"end\":787,\"start\":679}]'}}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# idx = 0\n","paper = pd.DataFrame(dataset['content'][idx])\n","paper"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":864},"id":"4FtJjYdnwibn","executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":25,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"e6f386cd-f0ae-4108-ba26-24422974819a"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                             source  \\\n","pdfurls             [https://www.aclweb.org/anthology/C16-1291.pdf]   \n","pdfsha                     7f12bd8efc6791399abdc587a1ca4f52776e2b88   \n","oainfo                                                         None   \n","abstract                                                        NaN   \n","author                                                          NaN   \n","authoraffiliation                                               NaN   \n","authorfirstname                                                 NaN   \n","authorlastname                                                  NaN   \n","bibauthor                                                       NaN   \n","bibauthorfirstname                                              NaN   \n","bibauthorlastname                                               NaN   \n","bibentry                                                        NaN   \n","bibref                                                          NaN   \n","bibtitle                                                        NaN   \n","bibvenue                                                        NaN   \n","figure                                                          NaN   \n","figurecaption                                                   NaN   \n","figureref                                                       NaN   \n","formula                                                         NaN   \n","paragraph                                                       NaN   \n","publisher                                                       NaN   \n","sectionheader                                                   NaN   \n","table                                                           NaN   \n","tableref                                                        NaN   \n","title                                                           NaN   \n","venue                                                           NaN   \n","\n","                                                                 text  \\\n","pdfurls             \\nNeural Machine Translation with Supervised A...   \n","pdfsha              \\nNeural Machine Translation with Supervised A...   \n","oainfo              \\nNeural Machine Translation with Supervised A...   \n","abstract            \\nNeural Machine Translation with Supervised A...   \n","author              \\nNeural Machine Translation with Supervised A...   \n","authoraffiliation   \\nNeural Machine Translation with Supervised A...   \n","authorfirstname     \\nNeural Machine Translation with Supervised A...   \n","authorlastname      \\nNeural Machine Translation with Supervised A...   \n","bibauthor           \\nNeural Machine Translation with Supervised A...   \n","bibauthorfirstname  \\nNeural Machine Translation with Supervised A...   \n","bibauthorlastname   \\nNeural Machine Translation with Supervised A...   \n","bibentry            \\nNeural Machine Translation with Supervised A...   \n","bibref              \\nNeural Machine Translation with Supervised A...   \n","bibtitle            \\nNeural Machine Translation with Supervised A...   \n","bibvenue            \\nNeural Machine Translation with Supervised A...   \n","figure              \\nNeural Machine Translation with Supervised A...   \n","figurecaption       \\nNeural Machine Translation with Supervised A...   \n","figureref           \\nNeural Machine Translation with Supervised A...   \n","formula             \\nNeural Machine Translation with Supervised A...   \n","paragraph           \\nNeural Machine Translation with Supervised A...   \n","publisher           \\nNeural Machine Translation with Supervised A...   \n","sectionheader       \\nNeural Machine Translation with Supervised A...   \n","table               \\nNeural Machine Translation with Supervised A...   \n","tableref            \\nNeural Machine Translation with Supervised A...   \n","title               \\nNeural Machine Translation with Supervised A...   \n","venue               \\nNeural Machine Translation with Supervised A...   \n","\n","                                                          annotations  \n","pdfurls                                                           NaN  \n","pdfsha                                                            NaN  \n","oainfo                                                            NaN  \n","abstract                                   [{\"end\":1918,\"start\":913}]  \n","author              [{\"end\":221,\"start\":75},{\"end\":355,\"start\":222...  \n","authoraffiliation   [{\"end\":220,\"start\":103},{\"end\":354,\"start\":23...  \n","authorfirstname     [{\"end\":80,\"start\":75},{\"end\":227,\"start\":222}...  \n","authorlastname      [{\"end\":84,\"start\":81},{\"end\":235,\"start\":228}...  \n","bibauthor           [{\"end\":28307,\"start\":28292},{\"end\":28322,\"sta...  \n","bibauthorfirstname  [{\"end\":28298,\"start\":28292},{\"end\":28313,\"sta...  \n","bibauthorlastname   [{\"end\":28305,\"start\":28299},{\"end\":28320,\"sta...  \n","bibentry            [{\"attributes\":{\"doi\":\"abs/1606.02006\",\"id\":\"b...  \n","bibref              [{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2057,\"sta...  \n","bibtitle            [{\"end\":28910,\"start\":28803},{\"end\":29235,\"sta...  \n","bibvenue            [{\"end\":29703,\"start\":29699},{\"end\":30069,\"sta...  \n","figure              [{\"attributes\":{\"id\":\"fig_0\"},\"end\":26750,\"sta...  \n","figurecaption       [{\"end\":26750,\"start\":26426},{\"end\":26822,\"sta...  \n","figureref           [{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4961,\"...  \n","formula             [{\"attributes\":{\"id\":\"formula_0\"},\"end\":6911,\"...  \n","paragraph           [{\"end\":2712,\"start\":1934},{\"end\":3955,\"start\"...  \n","publisher                                                        None  \n","sectionheader       [{\"attributes\":{\"n\":\"1\"},\"end\":1932,\"start\":19...  \n","table                                   [{\"end\":27443,\"start\":27097}]  \n","tableref            [{\"end\":16332,\"start\":16325},{\"end\":16483,\"sta...  \n","title                  [{\"end\":53,\"start\":1},{\"end\":677,\"start\":625}]  \n","venue                                       [{\"end\":787,\"start\":679}]  "],"text/html":["\n","  <div id=\"df-032f78fc-edcc-4871-addf-096f30c4719d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>text</th>\n","      <th>annotations</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pdfurls</th>\n","      <td>[https://www.aclweb.org/anthology/C16-1291.pdf]</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>pdfsha</th>\n","      <td>7f12bd8efc6791399abdc587a1ca4f52776e2b88</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>oainfo</th>\n","      <td>None</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>abstract</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":1918,\"start\":913}]</td>\n","    </tr>\n","    <tr>\n","      <th>author</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":221,\"start\":75},{\"end\":355,\"start\":222...</td>\n","    </tr>\n","    <tr>\n","      <th>authoraffiliation</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":220,\"start\":103},{\"end\":354,\"start\":23...</td>\n","    </tr>\n","    <tr>\n","      <th>authorfirstname</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":80,\"start\":75},{\"end\":227,\"start\":222}...</td>\n","    </tr>\n","    <tr>\n","      <th>authorlastname</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":84,\"start\":81},{\"end\":235,\"start\":228}...</td>\n","    </tr>\n","    <tr>\n","      <th>bibauthor</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":28307,\"start\":28292},{\"end\":28322,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>bibauthorfirstname</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":28298,\"start\":28292},{\"end\":28313,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>bibauthorlastname</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":28305,\"start\":28299},{\"end\":28320,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>bibentry</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"attributes\":{\"doi\":\"abs/1606.02006\",\"id\":\"b...</td>\n","    </tr>\n","    <tr>\n","      <th>bibref</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2057,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>bibtitle</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":28910,\"start\":28803},{\"end\":29235,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>bibvenue</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":29703,\"start\":29699},{\"end\":30069,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>figure</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26750,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>figurecaption</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":26750,\"start\":26426},{\"end\":26822,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>figureref</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4961,\"...</td>\n","    </tr>\n","    <tr>\n","      <th>formula</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6911,\"...</td>\n","    </tr>\n","    <tr>\n","      <th>paragraph</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":2712,\"start\":1934},{\"end\":3955,\"start\"...</td>\n","    </tr>\n","    <tr>\n","      <th>publisher</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>sectionheader</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"attributes\":{\"n\":\"1\"},\"end\":1932,\"start\":19...</td>\n","    </tr>\n","    <tr>\n","      <th>table</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":27443,\"start\":27097}]</td>\n","    </tr>\n","    <tr>\n","      <th>tableref</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":16332,\"start\":16325},{\"end\":16483,\"sta...</td>\n","    </tr>\n","    <tr>\n","      <th>title</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":53,\"start\":1},{\"end\":677,\"start\":625}]</td>\n","    </tr>\n","    <tr>\n","      <th>venue</th>\n","      <td>NaN</td>\n","      <td>\\nNeural Machine Translation with Supervised A...</td>\n","      <td>[{\"end\":787,\"start\":679}]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-032f78fc-edcc-4871-addf-096f30c4719d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-032f78fc-edcc-4871-addf-096f30c4719d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-032f78fc-edcc-4871-addf-096f30c4719d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4d343f35-fdec-4ee0-9022-fdd8057547d1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d343f35-fdec-4ee0-9022-fdd8057547d1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4d343f35-fdec-4ee0-9022-fdd8057547d1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# title\n","paper['text'][idx][1:53]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-HHCNL0MyKji","executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":24,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"1a2ba452-1b91-4f68-bdf9-85e21f2fd53e"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Neural Machine Translation with Supervised Attention'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# abstract\n","paper['text'][idx][913:1918]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"NeWAbKRWwpHI","executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":24,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"55f9168e-3410-4cf8-a64c-579eaf56004e"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The attention mechanism is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in alignment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.This work is licensed under a Creative Commons Attribution 4.0 International Licence.Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Throughout this paper, without the special statement, NMT means attention-based NMT. 2  We do agree that NMT is a supervised model with respect to translation rather than reordering.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# section header with annotation start and end index of full text\n","paper['annotations']['sectionheader']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"MMLLw4ucxeHZ","executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":23,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"10164e37-8b3c-4952-e611-529a1947a694"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[{\"attributes\":{\"n\":\"1\"},\"end\":1932,\"start\":1920},{\"attributes\":{\"n\":\"2\"},\"end\":6789,\"start\":6752},{\"attributes\":{\"n\":\"3\"},\"end\":9880,\"start\":9860},{\"attributes\":{\"n\":\"3.1\"},\"end\":10426,\"start\":10391},{\"attributes\":{\"n\":\"3.2\"},\"end\":11594,\"start\":11549},{\"attributes\":{\"n\":\"4\"},\"end\":13494,\"start\":13483},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":16083,\"start\":16052},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":17996,\"start\":17957},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":18032,\"start\":17999},{\"attributes\":{\"n\":\"4.2\"},\"end\":21383,\"start\":21339},{\"attributes\":{\"n\":\"5\"},\"end\":23132,\"start\":23120},{\"attributes\":{\"n\":\"6\"},\"end\":25797,\"start\":25787},{\"end\":26424,\"start\":26414},{\"end\":26762,\"start\":26752},{\"end\":26834,\"start\":26824}]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# get header from full text\n","for header in eval(paper['annotations']['sectionheader']):\n","  print(header, '\\t\\t', paper['text'][0][header['start']:header['end']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wioZ3jxlBU4w","executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":23,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"2f3ad90f-20ba-4e38-886d-1165c24cef2e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'attributes': {'n': '1'}, 'end': 1932, 'start': 1920} \t\t Introduction\n","{'attributes': {'n': '2'}, 'end': 6789, 'start': 6752} \t\t Revisiting Neural Machine Translation\n","{'attributes': {'n': '3'}, 'end': 9880, 'start': 9860} \t\t Supervised Attention\n","{'attributes': {'n': '3.1'}, 'end': 10426, 'start': 10391} \t\t Preprocessing Alignment Supervision\n","{'attributes': {'n': '3.2'}, 'end': 11594, 'start': 11549} \t\t Jointly Supervising Translation and Attention\n","{'attributes': {'n': '4'}, 'end': 13494, 'start': 13483} \t\t Experiments\n","{'attributes': {'n': '4.1.2'}, 'end': 16083, 'start': 16052} \t\t Settings on External Alignments\n","{'attributes': {'n': '4.1.3'}, 'end': 17996, 'start': 17957} \t\t Results on Large Scale Translation Task\n","{'attributes': {'n': '4.1.4'}, 'end': 18032, 'start': 17999} \t\t Results and Analysis on Alignment\n","{'attributes': {'n': '4.2'}, 'end': 21383, 'start': 21339} \t\t Results on the Low Resource Translation Task\n","{'attributes': {'n': '5'}, 'end': 23132, 'start': 23120} \t\t Related Work\n","{'attributes': {'n': '6'}, 'end': 25797, 'start': 25787} \t\t Conclusion\n","{'end': 26424, 'start': 26414} \t\t Figure 1 :\n","{'end': 26762, 'start': 26752} \t\t Figure 2 :\n","{'end': 26834, 'start': 26824} \t\t Figure 3 :\n"]}]},{"cell_type":"code","source":["# get paragraph content from full text\n","for paragraph in eval(paper['annotations']['paragraph']):\n","  print(paragraph, '\\t\\t', paper['text'][0][paragraph['start']:paragraph['end']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNjrdiXBFCdh","executionInfo":{"status":"ok","timestamp":1701227067861,"user_tz":-420,"elapsed":10,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"67b92e12-e482-4ba8-a79e-65f286679de6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'end': 2712, 'start': 1934} \t\t Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015;Sutskever et al., 2015). Generally, it relies on a recurrent neural network under the Encode-Decode framework: it firstly encodes a source sentence into context vectors and then generates its translation token-by-token, selecting from the target vocabulary. Among different variants of NMT, attention based NMT, which is the focus of this paper, 1 is attracting increasing interests in the community (Bahdanau et al., 2015;. One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance.\n","{'end': 3955, 'start': 2714} \t\t An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000;Dyer et al., 2013;Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment. 2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006;Koehn et al., 2007;Bisazza and Federico, 2016).\n","{'end': 5184, 'start': 3957} \t\t Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the conventional aligners delivers higher quality alignment, it is expected that the alignment in the supervised attention NMT will be improved leading to better end-to-end translation performance. One advantage of the proposed SA-NMT is that it implements the supervision of attention as a regularization in the joint training objective ( §3.2). Furthermore, since the attention variables lies in the middle of the entire network architecture rather than the top (as the translation variables (see Figure 1(b)), it serves to mitigate the vanishing gradient problem during the back-propagation, by adding supervision into the intermediate layers in the network (Szegedy et al., 2015). This paper makes the following contributions:\n","{'end': 5575, 'start': 5186} \t\t • It revisits the attention model from the point view of reordering ( §2), and propose a supervised attention for NMT that is supervised by statistical alignment models ( §3). The proposed approach is simple and easy to be implemented, and it is generally applicable to any attention-based NMT models, although in this case it is implemented on top of the model in (Bahdanau et al., 2015).\n","{'end': 6749, 'start': 5577} \t\t • On two Chinese-to-English translation tasks, it empirically shows that the proposed approach gives rise to improved performance ( §4): on a large scale task, it outperforms three baselines including a state-of-the-art Moses, and leads to improvements of up to 2.5 BLEU points over the strongest one in this paper; on a low resource task, it even obtains about 5 BLEU points over the attention based NMT system on which is it based. Suppose x = x 1 , x 2 , · · · , x m denotes a source sentence, y = y 1 , y 2 , · · · , y n a target sentence. In addition, let x <t = x 1 , x 2 , · · · , x t−1 denote a prefix of x. Neural Machine Translation (NMT) directly maps a source sentence into a target under an encode-decode framework. In the encoding stage, it uses two bidirectional recurrent neural networks to encode x into a sequence of vectors E x = E x 1 , E x 2 , · · · , E xm , with E x i representing the concatenation of two vectors for i th source word from two directional RNNs. In the decoding stage, it generates the target translation from the conditional probability over the pair of sequences x and y via a recurrent neural network parametrized by θ as follows:\n","{'end': 7551, 'start': 7000} \t\t where h t and c t respectively denote an RNN hidden state (i.e. a vector) and a context vector at timestep t; g is a transformation function mapping into a vector with dimension of the target vocabulary size; and [i] denotes the i th component of a vector. 3 Furthermore, h t = f (h t−1 , y t−1 , c t ) is defined by an activation function, i.e. a Gated Recurrent Unit (Chung et al., 2014); and the context vector c t is a dynamical source representation at timestep t, and calculated as the weighted sum of source encodings E x , i.e. c t = α t E x .\n","{'end': 7742, 'start': 7553} \t\t Here the weight α t implements an attention mechanism, and α t,i is the alignment probability of y t being aligned to x i . α t is derived through a feedforward neural network a as follows:\n","{'end': 8055, 'start': 7776} \t\t where a consists of two layers, the top one being a softmax layer. We skip the detailed definitions of a together with E x , f and g, and refer the readers to (Bahdanau et al., 2015) instead. 4 Figure 1(a) shows one slice of computational graph for NMT definition at time step t.\n","{'end': 8122, 'start': 8057} \t\t To train NMT, the following negative log-likelyhood is minimized:\n","{'end': 8263, 'start': 8151} \t\t where x i , y i is a bilingual sentence pair from a given training corpus, p(y i | x i ; θ) is as defined in Eq.\n","{'end': 8485, 'start': 8265} \t\t (1). Note that even though the training is conducted in a supervised manner with respect to translation, i.e., y are observable in Figure 1(a), the attention is learned in a unsupervised manner, since α is hidden. In Eq.\n","{'end': 9073, 'start': 8487} \t\t (2), α t is defined only on y t−1 , h t−1 and E x but not on the target word y t , as y t is unknown at the current timestep t − 1 during the testing. Therefore, at timestep t − 1, NMT firstly tries to calculate α t , through which NMT figures out those source words will be translated next, even though the next target word y t is unavailable. From this point of view, the attention mechanism plays a role in reordering and thus can be considered as a reordering model. Unlike this attention model, conventional alignment models define the alignment α directly over x and y as follows:\n","{'end': 9857, 'start': 9125} \t\t where F denotes a feature function over a pair of sentences x and y together with their word alignment α, and it is either a log-probability log p(y, α | x) for a generative model like IBM models (Brown et al., 1993) or a well-designed feature function for discriminative models (Liu and Sun, 2015). In order to infer α t , alignment models can readily use the entire y, of course including y t as well, thereby they can model the alignment between x and y more sufficiently. As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models. This may be a sign that the potential of attention-based NMT is limited in end-to-end translation.\n","{'end': 10388, 'start': 9882} \t\t In this section, we introduce supervised attention to improve the alignment, which may lead to better translation performance for NMT. 5 Our basic idea is simple: similar to conventional SMT, it firstly uses a conventional aligner to obtain the alignment on the training corpus; then it employs these alignment results as supervision to train the NMT. During testing, decoding proceeds in exactly the same manner as standard NMT, since there is no alignment supervision available for unseen test sentences.\n","{'end': 11546, 'start': 10428} \t\t As described in §2, the attention model outputs a soft alignment α, such that α t is a normalized probability distribution. In contrast, most aligners are typically oriented to grammar induction for conventional SMT, and they usually output 'hard' alignments, such as (Och and Ney, 2000). They only indicate whether a target word is aligned to a source word or not, and this might not correspond to a distribution for each target word. For example, one target word may align to multiple source words, or no source words at all. Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens 'eol' added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted asα.\n","{'end': 11694, 'start': 11596} \t\t We propose a soft constraint method to jointly supervise the translation and attention as follows:\n","{'end': 11775, 'start': 11745} \t\t where α i is as defined in Eq.\n","{'end': 11953, 'start': 11777} \t\t (1), ∆ is a loss function that penalizes the disagreement between α i and α i , and λ > 0 is a hyper-parameter that balances the preference between likelihood and disagreement.\n","{'end': 12601, 'start': 11955} \t\t In this way, we treat the attention variable α as an observable variable as shown in Figure 1(b), and this is different from the standard NMT as shown in Figure 1(a) in essence. Note that this training objective resembles to that in multi-task learning (Evgeniou and Pontil, 2004). Our supervised attention method has two further advantages: firstly, it is able to alleviate overfitting by means of the λ; and secondly it is easier to address the vanishing gradient problem by adding supervision into the intermediate layers of the entire network (Szegedy et al., 2015), because the supervision of α is more close to E x than y as in Figure 1(b).\n","{'end': 12722, 'start': 12603} \t\t In order to quantify the disagreement between α i andα i , three different methods are investigated in our experiments:\n","{'end': 12938, 'start': 12798} \t\t MSE is widely used as a loss for regression tasks (Lehmann and Casella, 1998), and it directly encourages α(θ) i m,n to be equal toα i m,n .\n","{'end': 12962, 'start': 12940} \t\t • Multiplication (MUL)\n","{'end': 13268, 'start': 13011} \t\t MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006;Cheng et al., 2016). Note that different from those in (Cheng et al., 2016),α is not a parametrized variable but a constant in this paper.\n","{'end': 13290, 'start': 13270} \t\t • Cross Entropy (CE)\n","{'end': 13480, 'start': 13339} \t\t Since for each t, α(θ) t is a distribution, it is natural to use CE as the metric to evaluate the disagreement (Rubinstein and Kroese, 2004).\n","{'end': 14254, 'start': 13496} \t\t We conducted experiments on two Chinese-to-English translation tasks: one is the NIST task oriented to NEWS domain, which is a large scale task and suitable to NMT; and the other is the speech translation oriented to travel domain, which is a low resource task and thus is very challenging for NMT. We used the case-insensitive BLEU4 to evaluate translation quality and adopted the multi-bleu.perl as its implementation. We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences). We compared the proposed approach with three strong baselines:\n","{'end': 14328, 'start': 14256} \t\t • Moses: a phrase-based machine translation system (Koehn et al., 2007);\n","{'end': 14438, 'start': 14330} \t\t • NMT1: an attention based NMT (Bahdanau et al., 2015) system at https://github.com/lisagroundhog/GroundHog;\n","{'end': 14541, 'start': 14440} \t\t • NMT2: another implementation of (Bahdanau et al., 2015) at https://github.com/nyu-dl/dl4mttutorial.\n","{'end': 14898, 'start': 14543} \t\t We developed the proposed approach based on NMT2, and denoted it as SA-NMT. We followed the standard pipeline to run Moses. GIZA++ with grow-diag-final-and was used to build the translation model. We trained a 5-gram target language model on the Gigaword corpus, and used a lexicalized distortion model. All experiments were run with the default settings.\n","{'end': 16049, 'start': 14900} \t\t To train NMT1, NMT2 and SA-NMT, we employed the same settings for fair comparison. Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in (Bahdanau et al., 2015) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, 6 the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by (Zeiler, 2012). Particularly for SA-NMT, we employed a conventional word aligner to obtain the word alignment on the training data before training SA-NMT. In this paper, we used two different aligners, which are fast align and GIZA++. We tuned the hyper-parameter λ to be 0.3 on the development set, to balance the preference between the translation and alignment. Training was conducted on a single Tesla K40 GPU machine. Each update took about 3.0 seconds for both NMT2 and SA-NMT, and 2.4 seconds for NMT1. Roughly, it took about 10 days to NMT2 to finish 300000 updates.\n","{'end': 16797, 'start': 16085} \t\t We implemented three different losses to supervise the attention as described in §3.2. To explore their behaviors on the development set, we employed the GIZA++ to generate the alignment on the training set prior to the training SA-NMT. In Table 1, we can see that MUL is better than MSE. Furthermore, CE performs best among all losses, and thus we adopt it for the following experiments.   Table 3: BLEU comparison for large scale translation task. The development set is nist02, and the test sets are nist05,nist06 and nist08. '*' denotes that SA-NMT is significantly better than Moses, NMT1 and NMT2 with p < 0.01. Note that Moses is trained with more bilingual sentences and an additional monolingual corpus.\n","{'end': 17954, 'start': 16799} \t\t In addition, we also run fast align to generate alignments as the supervision for SA-NMT and the results were reported in Table 2. We can see that GIZA++ performs slightly better than fast align and thus we fix the external aligner as GIZA++ in the following experiments. Figure 2 shows the learning curves of NMT2 and SA-NMT on the development set. We can see that NMT2 generally obtains higher BLEU as the increasing of updates before peaking at update of 150000, while it is unstable from then on. On the other hand, SA-NMT delivers much better BLEU for the beginning updates and performs more steadily along with the updates, although it takes more updates to reach the peaking point. Table 3 reports the main end-to-end translation results for the large scale task. We find that both standard NMT generally outperforms Moses except NMT1 on nist05. The proposed SA-NMT achieves significant and consistent improvements over all three baseline systems, and it obtains the averaged gains of 2.2 BLEU points on test sets over its direct baseline NMT2. It is clear from these results that our supervised attention mechanism is highly effective in practice.\n","{'end': 19429, 'start': 18034} \t\t As explained in §2, standard NMT can not use the target word information to predict its aligned source words, and thus might fail to predict the correct source words for some target words. For example, for the sentence in the training set in Figure 3 (a), NMT2 aligned 'following' to '皮诺契特 (gloss: pinochet)' rather than '继 (gloss: follow)', and worse still it aligned the word '.' to '在 (gloss: in)' rather than '。' even though this word is relatively easy to align correctly. In contrast, with the help of information from the target word itself, GIZA++ successfully aligned both 'following' and '.' to the expected source words (see Figure3(c)). With the alignment results from GIZA++ as supervision, we can see that our SA-NMT can imitate GIZA++ and thus align both words correctly. More importantly, for sentences in the unseen test set, like GIZA++, SA-NMT confidently aligned 'but' and '.' to their correct source words respectively as in Figure3(b), where NMT2 failed. It seems that SA-NMT can learn its alignment behavior from GIZA++, and subsequently apply the alignment abilities it has learned to unseen test sentences. , and (c) GIZA++ on two Chinese-English sentence pairs. The soft alignments of (c) is converted from hard alignment as in §3.1. The first row shows the alignments of the sentence pair from the training set while the second row shows the alignments from test sets.\n","{'end': 20649, 'start': 19431} \t\t Methods AER GIZA++ 30.6 * NMT2 50.6 SA-NMT 43.3 * Table 4: Results on word alignment task for the large scale data. The evaluation metric is Alignment Error Rate (AER). '*' denotes that the corresponding result is significanly better than NMT2 with p < 0.01. Table 4 shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in (Liu and Sun, 2015) as the test set. Following , we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. From Table 4, we can see clearly that standard NMT (NMT2) is far behind GIZA++ in alignment quality. This shows that it is possible and promising to supervise the attention with GIZA++. With the help from GIZA++, our supervised attention based NMT (SA-NMT) significantly reduces the AER, compared with the unsupervised counterpart (NMT2). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance.\n","{'end': 21336, 'start': 20651} \t\t Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013;Tamura et al., 2014).  Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. '*' denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p < 0.01.\n","{'end': 22452, 'start': 21385} \t\t For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out sets, respectively. We trained a 4-gram language model on the target side of training corpus for running Moses. For training all NMT systems, we employed the same settings as those in the large scale task, except that vocabulary size is 6000, batch size is 16, and the hyper-parameter λ = 1 for SA-NMT. Table 5 reports the final results. Firstly, we can see that both standard neural machine translation systems NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains much over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially.\n","{'end': 23117, 'start': 22454} \t\t While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs;  revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015;Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work.\n","{'end': 23947, 'start': 23134} \t\t Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs.\n","{'end': 24855, 'start': 23949} \t\t It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al. (2014) proposed a neural network method to learn a BTG reordering model. At the word level, Bisazza and Federico (2016) surveyed many word reordering models learned from alignment models for SMT, and there are some neural network based reordering models, such as (Zhang et al., 2016). Our work is inspired by these works in spirit, and it can be considered to be a recurrent neural network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs.\n","{'end': 25784, 'start': 24857} \t\t Supervising the attention variables for attention-based neural networks is pioneered by . On image caption task,  supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering.\n","{'end': 26412, 'start': 25799} \t\t It has been shown that attention mechanism in NMT is worse than conventional word alignment models in its alignment accuracy. This paper firstly provides an explanation for this by viewing the attention mechanism from the point view of reordering. Then it proposes a supervised attention for NMT with guidance from external conventional alignment models, inspired by the supervised reordering models in conventional SMT. Experiments on two Chinese-to-English translation tasks show that the proposed approach achieves better alignment results leading to significant gains relative to standard attention based NMT.\n"]}]},{"cell_type":"markdown","source":["### Get title, abstract, list of headers each paper all dataset"],"metadata":{"id":"zHXm6A7CGoYW"}},{"cell_type":"code","source":["# full loop all dataset\n","df_list = []\n","for i in tqdm(range(1,31)):\n","  dataset = pd.read_json(base_path + 'filter/dataset_'+str(i)+'.json')\n","\n","  # loop each paper\n","  for idx,row in dataset.iterrows():\n","    content_dict = {}\n","    corpusid = row['corpusid']\n","    content = row['content']\n","    content_annotation = content['annotations']\n","    content_text = content['text']\n","\n","    try:\n","      # get index from annotation\n","      # note: title and abstract from direct api are more accurate than full text annotation\n","      try:\n","        title_idx = eval(content_annotation['title'])[0]\n","        abstract_idx = eval(content_annotation['abstract'])[0]\n","      except:\n","        title_idx = False\n","        abstract_idx = False\n","\n","      section_idx = eval(content_annotation['sectionheader'])\n","\n","      # get content from text by slicing index\n","      title = content_text[title_idx['start']:title_idx['end']] if title_idx != False else None\n","      abstract = content_text[abstract_idx['start']:abstract_idx['end']] if abstract_idx != False else None\n","      section_list = [content_text[s['start']:s['end']] for s in section_idx ]\n","      section_dict = [{'raw_header': content_text[s['start']:s['end']],\n","                  'normalize_header': normalize_header(content_text[s['start']:s['end']]),\n","                  'start': s['start'],\n","                  'end': s['end']\n","                 }\n","                 for s in section_idx]\n","\n","      # update dict\n","      content_dict.update({'datasetId': i,\n","                          'corpusId': corpusid,\n","                          'title': title,\n","                          'abstract': abstract,\n","                          'sectionHeaderList': section_list,\n","                          'sectionHeaderDict': section_dict\n","                          })\n","      df_list.append(content_dict)\n","\n","    except Exception as e:\n","      # print(idx, corpusid, e)\n","\n","      # update dict\n","      content_dict.update({'datasetId': i,\n","                          'corpusId': corpusid,\n","                          'title': None,\n","                          'abstract': None,\n","                          'sectionHeaderList': [],\n","                          'sectionHeaderDict': []\n","                          })\n","      df_list.append(content_dict)\n","\n","# convert list to dataframe\n","df = pd.DataFrame(df_list)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324,"referenced_widgets":["625be81f804b43b78617791ce44dcdc1","05df3460751641daad28321af1ea67b1","3e35a2c378554fb3bc4e80fa00aedfaf","92faea01d91d40edbd5dd469f94648eb","f15b65ed26314601abe1a99c609de89a","f7ad821109ce4c0a86e225f5daff8882","e8e981cdd8b54c62aa25c112e9c6c7f7","ecec38339a744b3aa0f50008283aa91e","a93da34c97c443f198f6b9aa1845741f","a2a2c0f826564dac923838afe64d03de","4b2ba9d3a4ed4f4e93f7a25b0092344f"]},"id":"ZrJAfJbyFHM_","executionInfo":{"status":"ok","timestamp":1701227459945,"user_tz":-420,"elapsed":387960,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"d4a7bc5f-be28-4451-a13c-c9676fa19cdf"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/30 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625be81f804b43b78617791ce44dcdc1"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["   datasetId  corpusId                                              title  \\\n","0          1  13292366  Neural Machine Translation with Supervised Att...   \n","1          1  20527197  Spatial Variational Auto-Encoding via Matrix-V...   \n","2          1    116505             Few-Shot Adversarial Domain Adaptation   \n","3          1   4693065  Mix and match networks: encoder-decoder alignm...   \n","4          1  85498398                                               None   \n","\n","                                            abstract  \\\n","0  The attention mechanism is appealing for neura...   \n","1  The key idea of variational auto-encoders (VAE...   \n","2  This work provides a framework for addressing ...   \n","3  We address the problem of image translation be...   \n","4                                               None   \n","\n","                                   sectionHeaderList  \\\n","0  [Introduction, Revisiting Neural Machine Trans...   \n","1  [Introduction, Background and Related Work, Au...   \n","2  [Introduction, Related work, Few-shot adversar...   \n","3  [Introduction, Train, Related Work, .1. Multi-...   \n","4  [INTRODUCTION, RELATED WORKS, Real/Fake, YOUTU...   \n","\n","                                   sectionHeaderDict  \n","0  [{'raw_header': 'Introduction', 'normalize_hea...  \n","1  [{'raw_header': 'Introduction', 'normalize_hea...  \n","2  [{'raw_header': 'Introduction', 'normalize_hea...  \n","3  [{'raw_header': 'Introduction', 'normalize_hea...  \n","4  [{'raw_header': 'INTRODUCTION', 'normalize_hea...  "],"text/html":["\n","  <div id=\"df-1cbc1789-7800-481c-ac18-515570122e78\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datasetId</th>\n","      <th>corpusId</th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>sectionHeaderList</th>\n","      <th>sectionHeaderDict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>13292366</td>\n","      <td>Neural Machine Translation with Supervised Att...</td>\n","      <td>The attention mechanism is appealing for neura...</td>\n","      <td>[Introduction, Revisiting Neural Machine Trans...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>20527197</td>\n","      <td>Spatial Variational Auto-Encoding via Matrix-V...</td>\n","      <td>The key idea of variational auto-encoders (VAE...</td>\n","      <td>[Introduction, Background and Related Work, Au...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>116505</td>\n","      <td>Few-Shot Adversarial Domain Adaptation</td>\n","      <td>This work provides a framework for addressing ...</td>\n","      <td>[Introduction, Related work, Few-shot adversar...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>4693065</td>\n","      <td>Mix and match networks: encoder-decoder alignm...</td>\n","      <td>We address the problem of image translation be...</td>\n","      <td>[Introduction, Train, Related Work, .1. Multi-...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>85498398</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>[INTRODUCTION, RELATED WORKS, Real/Fake, YOUTU...</td>\n","      <td>[{'raw_header': 'INTRODUCTION', 'normalize_hea...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cbc1789-7800-481c-ac18-515570122e78')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1cbc1789-7800-481c-ac18-515570122e78 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1cbc1789-7800-481c-ac18-515570122e78');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-99ccb8a9-1443-4f5b-aea0-56a6d55f73ad\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99ccb8a9-1443-4f5b-aea0-56a6d55f73ad')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-99ccb8a9-1443-4f5b-aea0-56a6d55f73ad button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# deduplicate sectionHeaderList\n","dedup = []\n","for original_list in df['sectionHeaderList']:\n","  seen_list = set()\n","  deduplicated_list = [d for d in original_list if not (d in seen_list or seen_list.add(d))]\n","  dedup.append(deduplicated_list)\n","df['sectionHeaderList'] = dedup"],"metadata":{"id":"6Hf91CoFGOfd","executionInfo":{"status":"ok","timestamp":1701227464087,"user_tz":-420,"elapsed":4145,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# deduplicate sectionHeaderDict\n","dedup = []\n","for original_list in df['sectionHeaderDict']:\n","  # print(original_list)\n","  seen_entries = set()\n","  deduplicated_list = [d for d in original_list if not (tuple(d.items()) in seen_entries or seen_entries.add(tuple(d.items())))]\n","  dedup.append(deduplicated_list)\n","df['sectionHeaderDict'] = dedup"],"metadata":{"id":"KrG3ZjBNKCk-","executionInfo":{"status":"ok","timestamp":1701227472714,"user_tz":-420,"elapsed":8629,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# write result to file\n","df.to_csv(base_path + \"filtered_corpusid_content.csv\", index=False)"],"metadata":{"id":"V8RV_g5rVF2C","executionInfo":{"status":"ok","timestamp":1701227519905,"user_tz":-420,"elapsed":47192,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Get text of each focused header all dataset\n","(introduction, method, result and conclusion)"],"metadata":{"id":"eRbhbPHhHl1K"}},{"cell_type":"code","source":["df = pd.read_csv(base_path + \"filtered_corpusid_content.csv\")\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292},"id":"IfHsxb3C2pyg","executionInfo":{"status":"ok","timestamp":1701227537917,"user_tz":-420,"elapsed":18017,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"3093738c-11b3-48fd-ec00-b665f675422e"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   datasetId  corpusId                                              title  \\\n","0          1  13292366  Neural Machine Translation with Supervised Att...   \n","1          1  20527197  Spatial Variational Auto-Encoding via Matrix-V...   \n","2          1    116505             Few-Shot Adversarial Domain Adaptation   \n","3          1   4693065  Mix and match networks: encoder-decoder alignm...   \n","4          1  85498398                                                NaN   \n","\n","                                            abstract  \\\n","0  The attention mechanism is appealing for neura...   \n","1  The key idea of variational auto-encoders (VAE...   \n","2  This work provides a framework for addressing ...   \n","3  We address the problem of image translation be...   \n","4                                                NaN   \n","\n","                                   sectionHeaderList  \\\n","0  ['Introduction', 'Revisiting Neural Machine Tr...   \n","1  ['Introduction', 'Background and Related Work'...   \n","2  ['Introduction', 'Related work', 'Few-shot adv...   \n","3  ['Introduction', 'Train', 'Related Work', '.1....   \n","4  ['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...   \n","\n","                                   sectionHeaderDict  \n","0  [{'raw_header': 'Introduction', 'normalize_hea...  \n","1  [{'raw_header': 'Introduction', 'normalize_hea...  \n","2  [{'raw_header': 'Introduction', 'normalize_hea...  \n","3  [{'raw_header': 'Introduction', 'normalize_hea...  \n","4  [{'raw_header': 'INTRODUCTION', 'normalize_hea...  "],"text/html":["\n","  <div id=\"df-ad264140-e5ce-4b06-8ccb-1e4a7a52d427\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datasetId</th>\n","      <th>corpusId</th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>sectionHeaderList</th>\n","      <th>sectionHeaderDict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>13292366</td>\n","      <td>Neural Machine Translation with Supervised Att...</td>\n","      <td>The attention mechanism is appealing for neura...</td>\n","      <td>['Introduction', 'Revisiting Neural Machine Tr...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>20527197</td>\n","      <td>Spatial Variational Auto-Encoding via Matrix-V...</td>\n","      <td>The key idea of variational auto-encoders (VAE...</td>\n","      <td>['Introduction', 'Background and Related Work'...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>116505</td>\n","      <td>Few-Shot Adversarial Domain Adaptation</td>\n","      <td>This work provides a framework for addressing ...</td>\n","      <td>['Introduction', 'Related work', 'Few-shot adv...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>4693065</td>\n","      <td>Mix and match networks: encoder-decoder alignm...</td>\n","      <td>We address the problem of image translation be...</td>\n","      <td>['Introduction', 'Train', 'Related Work', '.1....</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>85498398</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...</td>\n","      <td>[{'raw_header': 'INTRODUCTION', 'normalize_hea...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad264140-e5ce-4b06-8ccb-1e4a7a52d427')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ad264140-e5ce-4b06-8ccb-1e4a7a52d427 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ad264140-e5ce-4b06-8ccb-1e4a7a52d427');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4f0dc368-2704-47c1-91d6-e1d3bb724937\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f0dc368-2704-47c1-91d6-e1d3bb724937')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4f0dc368-2704-47c1-91d6-e1d3bb724937 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# full loop all dataset\n","curr_dataset = 0\n","dataset_list = []\n","\n","for i, dataset_id in tqdm(enumerate(df['datasetId']), total=len(df)):\n","  if dataset_id != curr_dataset:\n","    print(\"dataset_\"+str(dataset_id)+'.json')\n","    dataset = pd.read_json(base_path + 'filter/dataset_'+str(dataset_id)+'.json')\n","    curr_dataset = dataset_id\n","\n","  # select only current corpusid at row i\n","  corpus = dataset[dataset['corpusid']==df['corpusId'][i]]\n","  pdf_url = corpus['content'].values[0]['source']['pdfurls']\n","  section_header_dict = eval(df['sectionHeaderDict'][i])\n","\n","  text_dict = {'corpus_id': df['corpusId'][i],\n","               'pdf_url': None if pdf_url == None else pdf_url[0]}\n","\n","  try:\n","    full_text = corpus['content'].values[0]['text']\n","    paragraph_anno = eval(corpus['content'].values[0]['annotations']['paragraph'])\n","\n","    # get text of each header\n","    for j in range(len(section_header_dict)):\n","      # get index of paragraph corresponding to header (after the end of current header and before the start of next header)\n","      end_curr_idx = section_header_dict[j]['end']\n","      try:\n","        start_next_idx = section_header_dict[j+1]['start']\n","      except:\n","        start_next_idx = len(full_text) # max length of full text\n","\n","      # check if normalize_header is in our focused headers, otherwise ignore it and continue next header\n","      if section_header_dict[j]['normalize_header'] in ['introduction', 'method', 'result', 'conclusion']:\n","        text = \"\"\n","\n","        for p_anno in paragraph_anno:\n","          if p_anno['start'] >= end_curr_idx and p_anno['end'] <= start_next_idx:\n","            text = \"\\n\".join([text, full_text[p_anno['start']:p_anno['end']]]) if text != \"\" else full_text[p_anno['start']:p_anno['end']]\n","\n","        # if normalize_header is not in key of dictionary then add new key = normalize_header and value = text, otherwise concat text to existing value\n","        if section_header_dict[j]['normalize_header'] not in text_dict:\n","          text_dict[section_header_dict[j]['normalize_header']] = text\n","        else:\n","          text_dict[section_header_dict[j]['normalize_header']] += (\"\\n\"+text)\n","\n","    dataset_list.append(text_dict)\n","  except Exception as e:\n","    dataset_list.append(text_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":570,"referenced_widgets":["ea1765f1082c4ec79fa449b9a563c6b9","378eb620ee6c4261888ecd6f5ec2f7af","10771ebd60c94ab6a0bb5d1751d1efe6","7e6c860fbe4d4aa89c1fd7d2136f46d5","685e79d3577e403ca49c4e756dc54ec3","034d582f6c504ddf9846693517c4f9ec","ddd3194014a84405b02e0600f4fe0b95","f83add10bb87464db710282701f5681f","7f2a1083d6fb4fa58121229ae0055e7c","5da3968c386645e8b62b425744401318","ea6821f4ad064171ba9d3a2cfae3c53f"]},"id":"ATe3n53THkHF","executionInfo":{"status":"ok","timestamp":1701228082312,"user_tz":-420,"elapsed":544398,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"d02be1d5-5afd-4985-eea8-44f4f902c1b3"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/171123 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea1765f1082c4ec79fa449b9a563c6b9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dataset_1.json\n","dataset_2.json\n","dataset_3.json\n","dataset_4.json\n","dataset_5.json\n","dataset_6.json\n","dataset_7.json\n","dataset_8.json\n","dataset_9.json\n","dataset_10.json\n","dataset_11.json\n","dataset_12.json\n","dataset_13.json\n","dataset_14.json\n","dataset_15.json\n","dataset_16.json\n","dataset_17.json\n","dataset_18.json\n","dataset_19.json\n","dataset_20.json\n","dataset_21.json\n","dataset_22.json\n","dataset_23.json\n","dataset_24.json\n","dataset_25.json\n","dataset_26.json\n","dataset_27.json\n","dataset_28.json\n","dataset_29.json\n","dataset_30.json\n"]}]},{"cell_type":"code","source":["# convert dict to dataframe\n","df_text = pd.DataFrame(dataset_list)\n","df_text.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292},"id":"gfbzFiVLjZ79","executionInfo":{"status":"ok","timestamp":1701228083126,"user_tz":-420,"elapsed":833,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"86abd250-a8e8-4ac8-f173-8a203b07b427"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   corpus_id                                        pdf_url  \\\n","0   13292366  https://www.aclweb.org/anthology/C16-1291.pdf   \n","1   20527197         https://arxiv.org/pdf/1705.06821v1.pdf   \n","2     116505         https://arxiv.org/pdf/1711.02536v1.pdf   \n","3    4693065         https://arxiv.org/pdf/1804.02199v1.pdf   \n","4   85498398         https://arxiv.org/pdf/1903.10195v1.pdf   \n","\n","                                        introduction  \\\n","0  Neural Machine Translation (NMT) has achieved ...   \n","1  The mathematical and computational modeling of...   \n","2  As deep learning approaches have gained promin...   \n","3  Image-to-image translations (or simply image t...   \n","4  Audio and visual signals are the most common m...   \n","\n","                                              result  \\\n","0  We conducted experiments on two Chinese-to-Eng...   \n","1  In this section, we report experimental result...   \n","2  We present results using the Office dataset [4...   \n","3  To the best of our knowledge there is no exist...   \n","4  Model training: The Wav2Pix model was trained ...   \n","\n","                                          conclusion  \\\n","0  It has been shown that attention mechanism in ...   \n","1  In this work, we propose spatial VAEs for imag...   \n","2  We have introduced a deep model combining a cl...   \n","3  In this paper we introduce the problem of zero...   \n","4  In this work we introduced a simple yet effect...   \n","\n","                                              method  \n","0                                                NaN  \n","1                                                NaN  \n","2                                                NaN  \n","3  We compare the results of our mix and match ne...  \n","4  Since our goal is to train a GAN conditioned o...  "],"text/html":["\n","  <div id=\"df-c5fa7c4d-c296-4f0b-894c-4e75a8d9373c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corpus_id</th>\n","      <th>pdf_url</th>\n","      <th>introduction</th>\n","      <th>result</th>\n","      <th>conclusion</th>\n","      <th>method</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13292366</td>\n","      <td>https://www.aclweb.org/anthology/C16-1291.pdf</td>\n","      <td>Neural Machine Translation (NMT) has achieved ...</td>\n","      <td>We conducted experiments on two Chinese-to-Eng...</td>\n","      <td>It has been shown that attention mechanism in ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20527197</td>\n","      <td>https://arxiv.org/pdf/1705.06821v1.pdf</td>\n","      <td>The mathematical and computational modeling of...</td>\n","      <td>In this section, we report experimental result...</td>\n","      <td>In this work, we propose spatial VAEs for imag...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>116505</td>\n","      <td>https://arxiv.org/pdf/1711.02536v1.pdf</td>\n","      <td>As deep learning approaches have gained promin...</td>\n","      <td>We present results using the Office dataset [4...</td>\n","      <td>We have introduced a deep model combining a cl...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4693065</td>\n","      <td>https://arxiv.org/pdf/1804.02199v1.pdf</td>\n","      <td>Image-to-image translations (or simply image t...</td>\n","      <td>To the best of our knowledge there is no exist...</td>\n","      <td>In this paper we introduce the problem of zero...</td>\n","      <td>We compare the results of our mix and match ne...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>85498398</td>\n","      <td>https://arxiv.org/pdf/1903.10195v1.pdf</td>\n","      <td>Audio and visual signals are the most common m...</td>\n","      <td>Model training: The Wav2Pix model was trained ...</td>\n","      <td>In this work we introduced a simple yet effect...</td>\n","      <td>Since our goal is to train a GAN conditioned o...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5fa7c4d-c296-4f0b-894c-4e75a8d9373c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c5fa7c4d-c296-4f0b-894c-4e75a8d9373c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c5fa7c4d-c296-4f0b-894c-4e75a8d9373c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a7edea0a-c846-4294-8658-c5fd72878dfe\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7edea0a-c846-4294-8658-c5fd72878dfe')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a7edea0a-c846-4294-8658-c5fd72878dfe button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### Merge all dataframes"],"metadata":{"id":"NKbUqru3wOUJ"}},{"cell_type":"code","source":["df_all = pd.merge(df, df_text, left_on=\"corpusId\", right_on=\"corpus_id\", how=\"left\")\n","df_all.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"id":"eXdefM6Ou1gJ","executionInfo":{"status":"ok","timestamp":1701228083588,"user_tz":-420,"elapsed":467,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"cee049fe-41ed-408e-afbc-4b36b7957e3e"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   datasetId  corpusId                                              title  \\\n","0          1  13292366  Neural Machine Translation with Supervised Att...   \n","1          1  20527197  Spatial Variational Auto-Encoding via Matrix-V...   \n","2          1    116505             Few-Shot Adversarial Domain Adaptation   \n","3          1   4693065  Mix and match networks: encoder-decoder alignm...   \n","4          1  85498398                                                NaN   \n","\n","                                            abstract  \\\n","0  The attention mechanism is appealing for neura...   \n","1  The key idea of variational auto-encoders (VAE...   \n","2  This work provides a framework for addressing ...   \n","3  We address the problem of image translation be...   \n","4                                                NaN   \n","\n","                                   sectionHeaderList  \\\n","0  ['Introduction', 'Revisiting Neural Machine Tr...   \n","1  ['Introduction', 'Background and Related Work'...   \n","2  ['Introduction', 'Related work', 'Few-shot adv...   \n","3  ['Introduction', 'Train', 'Related Work', '.1....   \n","4  ['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...   \n","\n","                                   sectionHeaderDict  corpus_id  \\\n","0  [{'raw_header': 'Introduction', 'normalize_hea...   13292366   \n","1  [{'raw_header': 'Introduction', 'normalize_hea...   20527197   \n","2  [{'raw_header': 'Introduction', 'normalize_hea...     116505   \n","3  [{'raw_header': 'Introduction', 'normalize_hea...    4693065   \n","4  [{'raw_header': 'INTRODUCTION', 'normalize_hea...   85498398   \n","\n","                                         pdf_url  \\\n","0  https://www.aclweb.org/anthology/C16-1291.pdf   \n","1         https://arxiv.org/pdf/1705.06821v1.pdf   \n","2         https://arxiv.org/pdf/1711.02536v1.pdf   \n","3         https://arxiv.org/pdf/1804.02199v1.pdf   \n","4         https://arxiv.org/pdf/1903.10195v1.pdf   \n","\n","                                        introduction  \\\n","0  Neural Machine Translation (NMT) has achieved ...   \n","1  The mathematical and computational modeling of...   \n","2  As deep learning approaches have gained promin...   \n","3  Image-to-image translations (or simply image t...   \n","4  Audio and visual signals are the most common m...   \n","\n","                                              result  \\\n","0  We conducted experiments on two Chinese-to-Eng...   \n","1  In this section, we report experimental result...   \n","2  We present results using the Office dataset [4...   \n","3  To the best of our knowledge there is no exist...   \n","4  Model training: The Wav2Pix model was trained ...   \n","\n","                                          conclusion  \\\n","0  It has been shown that attention mechanism in ...   \n","1  In this work, we propose spatial VAEs for imag...   \n","2  We have introduced a deep model combining a cl...   \n","3  In this paper we introduce the problem of zero...   \n","4  In this work we introduced a simple yet effect...   \n","\n","                                              method  \n","0                                                NaN  \n","1                                                NaN  \n","2                                                NaN  \n","3  We compare the results of our mix and match ne...  \n","4  Since our goal is to train a GAN conditioned o...  "],"text/html":["\n","  <div id=\"df-3d6beca5-e8ba-4265-9421-1a0d852633ba\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datasetId</th>\n","      <th>corpusId</th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>sectionHeaderList</th>\n","      <th>sectionHeaderDict</th>\n","      <th>corpus_id</th>\n","      <th>pdf_url</th>\n","      <th>introduction</th>\n","      <th>result</th>\n","      <th>conclusion</th>\n","      <th>method</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>13292366</td>\n","      <td>Neural Machine Translation with Supervised Att...</td>\n","      <td>The attention mechanism is appealing for neura...</td>\n","      <td>['Introduction', 'Revisiting Neural Machine Tr...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>13292366</td>\n","      <td>https://www.aclweb.org/anthology/C16-1291.pdf</td>\n","      <td>Neural Machine Translation (NMT) has achieved ...</td>\n","      <td>We conducted experiments on two Chinese-to-Eng...</td>\n","      <td>It has been shown that attention mechanism in ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>20527197</td>\n","      <td>Spatial Variational Auto-Encoding via Matrix-V...</td>\n","      <td>The key idea of variational auto-encoders (VAE...</td>\n","      <td>['Introduction', 'Background and Related Work'...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>20527197</td>\n","      <td>https://arxiv.org/pdf/1705.06821v1.pdf</td>\n","      <td>The mathematical and computational modeling of...</td>\n","      <td>In this section, we report experimental result...</td>\n","      <td>In this work, we propose spatial VAEs for imag...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>116505</td>\n","      <td>Few-Shot Adversarial Domain Adaptation</td>\n","      <td>This work provides a framework for addressing ...</td>\n","      <td>['Introduction', 'Related work', 'Few-shot adv...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>116505</td>\n","      <td>https://arxiv.org/pdf/1711.02536v1.pdf</td>\n","      <td>As deep learning approaches have gained promin...</td>\n","      <td>We present results using the Office dataset [4...</td>\n","      <td>We have introduced a deep model combining a cl...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>4693065</td>\n","      <td>Mix and match networks: encoder-decoder alignm...</td>\n","      <td>We address the problem of image translation be...</td>\n","      <td>['Introduction', 'Train', 'Related Work', '.1....</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>4693065</td>\n","      <td>https://arxiv.org/pdf/1804.02199v1.pdf</td>\n","      <td>Image-to-image translations (or simply image t...</td>\n","      <td>To the best of our knowledge there is no exist...</td>\n","      <td>In this paper we introduce the problem of zero...</td>\n","      <td>We compare the results of our mix and match ne...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>85498398</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...</td>\n","      <td>[{'raw_header': 'INTRODUCTION', 'normalize_hea...</td>\n","      <td>85498398</td>\n","      <td>https://arxiv.org/pdf/1903.10195v1.pdf</td>\n","      <td>Audio and visual signals are the most common m...</td>\n","      <td>Model training: The Wav2Pix model was trained ...</td>\n","      <td>In this work we introduced a simple yet effect...</td>\n","      <td>Since our goal is to train a GAN conditioned o...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d6beca5-e8ba-4265-9421-1a0d852633ba')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3d6beca5-e8ba-4265-9421-1a0d852633ba button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3d6beca5-e8ba-4265-9421-1a0d852633ba');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ab9906a1-2087-4cd4-8308-a1910ab2251b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab9906a1-2087-4cd4-8308-a1910ab2251b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ab9906a1-2087-4cd4-8308-a1910ab2251b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# read input file from API\n","df_corpus = pd.read_csv(base_path + 'filtered_corpusid_paperid.csv')\n","df_corpus.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":694},"id":"b27oa10fw21s","executionInfo":{"status":"ok","timestamp":1701228135210,"user_tz":-420,"elapsed":51625,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"dd296049-360d-4fad-ec70-3e35524ed053"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                    paperId   corpusId  \\\n","0  000009f81ab69d928987bbb747568fed52e15617  231684524   \n","1  00000d4e56df23367226d9692feedc579ce51f89  222417077   \n","2  000012f410cbb7b7080e4a0fbe4c6c24198df9cd  240002945   \n","3  000022c6542f1a198e10ce906c7e2f8a0bfa9e00  250182821   \n","4  00002ea837f4c849477dd2706f676f6dba8d15d1  249993357   \n","\n","                                               title  \\\n","0  Optimal Reactive Power Dispatch for Voltage Se...   \n","1  P2P Energy Trading based on Blockchain and Cus...   \n","2  Scaling Analysis of Crime Rate with Large Scal...   \n","3  A simple method for the preliminary analysis a...   \n","4  A Masked Facial Landmarks Localization Method ...   \n","\n","                                            abstract  year  referenceCount  \\\n","0  The optimal reactive power dispatch (ORPD) is ...  2020              12   \n","1  In this paper, we study a local P2P energy tra...  2020              13   \n","2  Crime is a major threat to social security. Ju...  2021               0   \n","3  The vast multitude of LiDAR systems currently ...  2022              16   \n","4  At present, it has become normal for people to...  2022              18   \n","\n","   citationCount  influentialCitationCount publicationDate  \\\n","0              8                         0      2020-02-18   \n","1              0                         0      2020-09-01   \n","2              0                         0      2021-08-17   \n","3              2                         0      2022-05-16   \n","4              0                         0      2022-02-25   \n","\n","                                             authors  \n","0  [{'authorId': '2342313', 'name': 'R. Roy'}, {'...  \n","1  [{'authorId': '1490931829', 'name': 'Tao Chen'...  \n","2  [{'authorId': '2152912295', 'name': 'Jing Li'}...  \n","3  [{'authorId': '2044359322', 'name': 'Davide Ca...  \n","4  [{'authorId': '2173007353', 'name': 'Yiwen Che...  "],"text/html":["\n","  <div id=\"df-f8f88103-c839-4eb3-9f1b-eacd016c5e11\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paperId</th>\n","      <th>corpusId</th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>year</th>\n","      <th>referenceCount</th>\n","      <th>citationCount</th>\n","      <th>influentialCitationCount</th>\n","      <th>publicationDate</th>\n","      <th>authors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000009f81ab69d928987bbb747568fed52e15617</td>\n","      <td>231684524</td>\n","      <td>Optimal Reactive Power Dispatch for Voltage Se...</td>\n","      <td>The optimal reactive power dispatch (ORPD) is ...</td>\n","      <td>2020</td>\n","      <td>12</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>2020-02-18</td>\n","      <td>[{'authorId': '2342313', 'name': 'R. Roy'}, {'...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00000d4e56df23367226d9692feedc579ce51f89</td>\n","      <td>222417077</td>\n","      <td>P2P Energy Trading based on Blockchain and Cus...</td>\n","      <td>In this paper, we study a local P2P energy tra...</td>\n","      <td>2020</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2020-09-01</td>\n","      <td>[{'authorId': '1490931829', 'name': 'Tao Chen'...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000012f410cbb7b7080e4a0fbe4c6c24198df9cd</td>\n","      <td>240002945</td>\n","      <td>Scaling Analysis of Crime Rate with Large Scal...</td>\n","      <td>Crime is a major threat to social security. Ju...</td>\n","      <td>2021</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2021-08-17</td>\n","      <td>[{'authorId': '2152912295', 'name': 'Jing Li'}...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>000022c6542f1a198e10ce906c7e2f8a0bfa9e00</td>\n","      <td>250182821</td>\n","      <td>A simple method for the preliminary analysis a...</td>\n","      <td>The vast multitude of LiDAR systems currently ...</td>\n","      <td>2022</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2022-05-16</td>\n","      <td>[{'authorId': '2044359322', 'name': 'Davide Ca...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00002ea837f4c849477dd2706f676f6dba8d15d1</td>\n","      <td>249993357</td>\n","      <td>A Masked Facial Landmarks Localization Method ...</td>\n","      <td>At present, it has become normal for people to...</td>\n","      <td>2022</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2022-02-25</td>\n","      <td>[{'authorId': '2173007353', 'name': 'Yiwen Che...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8f88103-c839-4eb3-9f1b-eacd016c5e11')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f8f88103-c839-4eb3-9f1b-eacd016c5e11 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f8f88103-c839-4eb3-9f1b-eacd016c5e11');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a583ed58-deb3-4aaf-8aec-64b9f855386e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a583ed58-deb3-4aaf-8aec-64b9f855386e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a583ed58-deb3-4aaf-8aec-64b9f855386e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["df_all = pd.merge(df_all, df_corpus, left_on=\"corpusId\", right_on=\"corpusId\", how=\"left\")\n","df_all.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":672},"id":"p5FAwDq5wsl5","executionInfo":{"status":"ok","timestamp":1701228136875,"user_tz":-420,"elapsed":1668,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"81ab955d-6b94-4ad8-f811-105d7badd310"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   datasetId  corpusId                                            title_x  \\\n","0          1  13292366  Neural Machine Translation with Supervised Att...   \n","1          1  20527197  Spatial Variational Auto-Encoding via Matrix-V...   \n","2          1    116505             Few-Shot Adversarial Domain Adaptation   \n","3          1   4693065  Mix and match networks: encoder-decoder alignm...   \n","4          1  85498398                                                NaN   \n","\n","                                          abstract_x  \\\n","0  The attention mechanism is appealing for neura...   \n","1  The key idea of variational auto-encoders (VAE...   \n","2  This work provides a framework for addressing ...   \n","3  We address the problem of image translation be...   \n","4                                                NaN   \n","\n","                                   sectionHeaderList  \\\n","0  ['Introduction', 'Revisiting Neural Machine Tr...   \n","1  ['Introduction', 'Background and Related Work'...   \n","2  ['Introduction', 'Related work', 'Few-shot adv...   \n","3  ['Introduction', 'Train', 'Related Work', '.1....   \n","4  ['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...   \n","\n","                                   sectionHeaderDict  corpus_id  \\\n","0  [{'raw_header': 'Introduction', 'normalize_hea...   13292366   \n","1  [{'raw_header': 'Introduction', 'normalize_hea...   20527197   \n","2  [{'raw_header': 'Introduction', 'normalize_hea...     116505   \n","3  [{'raw_header': 'Introduction', 'normalize_hea...    4693065   \n","4  [{'raw_header': 'INTRODUCTION', 'normalize_hea...   85498398   \n","\n","                                         pdf_url  \\\n","0  https://www.aclweb.org/anthology/C16-1291.pdf   \n","1         https://arxiv.org/pdf/1705.06821v1.pdf   \n","2         https://arxiv.org/pdf/1711.02536v1.pdf   \n","3         https://arxiv.org/pdf/1804.02199v1.pdf   \n","4         https://arxiv.org/pdf/1903.10195v1.pdf   \n","\n","                                        introduction  \\\n","0  Neural Machine Translation (NMT) has achieved ...   \n","1  The mathematical and computational modeling of...   \n","2  As deep learning approaches have gained promin...   \n","3  Image-to-image translations (or simply image t...   \n","4  Audio and visual signals are the most common m...   \n","\n","                                              result  ...  \\\n","0  We conducted experiments on two Chinese-to-Eng...  ...   \n","1  In this section, we report experimental result...  ...   \n","2  We present results using the Office dataset [4...  ...   \n","3  To the best of our knowledge there is no exist...  ...   \n","4  Model training: The Wav2Pix model was trained ...  ...   \n","\n","                                              method  \\\n","0                                                NaN   \n","1                                                NaN   \n","2                                                NaN   \n","3  We compare the results of our mix and match ne...   \n","4  Since our goal is to train a GAN conditioned o...   \n","\n","                                    paperId  \\\n","0  7f12bd8efc6791399abdc587a1ca4f52776e2b88   \n","1  f67afec4226aba674e786698b39b85b124945ddd   \n","2  1c60ac28884d2414efcf0eec90561fae2a377311   \n","3  18a429184af3d181e2de53d60a38e0314c25db5b   \n","4  8e93efe94dcd8d5de9c5efc7961c2448a6759b8c   \n","\n","                                             title_y  \\\n","0  Neural Machine Translation with Supervised Att...   \n","1  Spatial Variational Auto-Encoding via Matrix-V...   \n","2             Few-Shot Adversarial Domain Adaptation   \n","3  Mix and Match Networks: Encoder-Decoder Alignm...   \n","4  Wav2Pix: Speech-conditioned Face Generation Us...   \n","\n","                                          abstract_y    year  referenceCount  \\\n","0  The attention mechanism is appealing for neura...  2016.0            32.0   \n","1  The key idea of variational auto-encoders (VAE...  2017.0            32.0   \n","2  This work provides a framework for addressing ...  2017.0            64.0   \n","3  We address the problem of image translation be...  2018.0            38.0   \n","4  Speech is a rich biometric signal that contain...  2019.0            31.0   \n","\n","   citationCount  influentialCitationCount  publicationDate  \\\n","0          141.0                      12.0       2016-09-14   \n","1            4.0                       0.0       2017-05-18   \n","2          335.0                      40.0       2017-11-01   \n","3           32.0                       1.0       2018-04-06   \n","4           67.0                       5.0       2019-03-25   \n","\n","                                             authors  \n","0  [{'authorId': '2978364', 'name': 'Lemao Liu'},...  \n","1  [{'authorId': '8492168', 'name': 'Zhengyang Wa...  \n","2  [{'authorId': '2897426', 'name': 'Saeid Motiia...  \n","3  [{'authorId': '2107916962', 'name': 'Yaxing Wa...  \n","4  [{'authorId': '1381525702', 'name': 'A. Duarte...  \n","\n","[5 rows x 21 columns]"],"text/html":["\n","  <div id=\"df-44cb08fd-6293-4b62-a422-e3d8d8bc4957\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datasetId</th>\n","      <th>corpusId</th>\n","      <th>title_x</th>\n","      <th>abstract_x</th>\n","      <th>sectionHeaderList</th>\n","      <th>sectionHeaderDict</th>\n","      <th>corpus_id</th>\n","      <th>pdf_url</th>\n","      <th>introduction</th>\n","      <th>result</th>\n","      <th>...</th>\n","      <th>method</th>\n","      <th>paperId</th>\n","      <th>title_y</th>\n","      <th>abstract_y</th>\n","      <th>year</th>\n","      <th>referenceCount</th>\n","      <th>citationCount</th>\n","      <th>influentialCitationCount</th>\n","      <th>publicationDate</th>\n","      <th>authors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>13292366</td>\n","      <td>Neural Machine Translation with Supervised Att...</td>\n","      <td>The attention mechanism is appealing for neura...</td>\n","      <td>['Introduction', 'Revisiting Neural Machine Tr...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>13292366</td>\n","      <td>https://www.aclweb.org/anthology/C16-1291.pdf</td>\n","      <td>Neural Machine Translation (NMT) has achieved ...</td>\n","      <td>We conducted experiments on two Chinese-to-Eng...</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>7f12bd8efc6791399abdc587a1ca4f52776e2b88</td>\n","      <td>Neural Machine Translation with Supervised Att...</td>\n","      <td>The attention mechanism is appealing for neura...</td>\n","      <td>2016.0</td>\n","      <td>32.0</td>\n","      <td>141.0</td>\n","      <td>12.0</td>\n","      <td>2016-09-14</td>\n","      <td>[{'authorId': '2978364', 'name': 'Lemao Liu'},...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>20527197</td>\n","      <td>Spatial Variational Auto-Encoding via Matrix-V...</td>\n","      <td>The key idea of variational auto-encoders (VAE...</td>\n","      <td>['Introduction', 'Background and Related Work'...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>20527197</td>\n","      <td>https://arxiv.org/pdf/1705.06821v1.pdf</td>\n","      <td>The mathematical and computational modeling of...</td>\n","      <td>In this section, we report experimental result...</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>f67afec4226aba674e786698b39b85b124945ddd</td>\n","      <td>Spatial Variational Auto-Encoding via Matrix-V...</td>\n","      <td>The key idea of variational auto-encoders (VAE...</td>\n","      <td>2017.0</td>\n","      <td>32.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>2017-05-18</td>\n","      <td>[{'authorId': '8492168', 'name': 'Zhengyang Wa...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>116505</td>\n","      <td>Few-Shot Adversarial Domain Adaptation</td>\n","      <td>This work provides a framework for addressing ...</td>\n","      <td>['Introduction', 'Related work', 'Few-shot adv...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>116505</td>\n","      <td>https://arxiv.org/pdf/1711.02536v1.pdf</td>\n","      <td>As deep learning approaches have gained promin...</td>\n","      <td>We present results using the Office dataset [4...</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>1c60ac28884d2414efcf0eec90561fae2a377311</td>\n","      <td>Few-Shot Adversarial Domain Adaptation</td>\n","      <td>This work provides a framework for addressing ...</td>\n","      <td>2017.0</td>\n","      <td>64.0</td>\n","      <td>335.0</td>\n","      <td>40.0</td>\n","      <td>2017-11-01</td>\n","      <td>[{'authorId': '2897426', 'name': 'Saeid Motiia...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>4693065</td>\n","      <td>Mix and match networks: encoder-decoder alignm...</td>\n","      <td>We address the problem of image translation be...</td>\n","      <td>['Introduction', 'Train', 'Related Work', '.1....</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>4693065</td>\n","      <td>https://arxiv.org/pdf/1804.02199v1.pdf</td>\n","      <td>Image-to-image translations (or simply image t...</td>\n","      <td>To the best of our knowledge there is no exist...</td>\n","      <td>...</td>\n","      <td>We compare the results of our mix and match ne...</td>\n","      <td>18a429184af3d181e2de53d60a38e0314c25db5b</td>\n","      <td>Mix and Match Networks: Encoder-Decoder Alignm...</td>\n","      <td>We address the problem of image translation be...</td>\n","      <td>2018.0</td>\n","      <td>38.0</td>\n","      <td>32.0</td>\n","      <td>1.0</td>\n","      <td>2018-04-06</td>\n","      <td>[{'authorId': '2107916962', 'name': 'Yaxing Wa...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>85498398</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...</td>\n","      <td>[{'raw_header': 'INTRODUCTION', 'normalize_hea...</td>\n","      <td>85498398</td>\n","      <td>https://arxiv.org/pdf/1903.10195v1.pdf</td>\n","      <td>Audio and visual signals are the most common m...</td>\n","      <td>Model training: The Wav2Pix model was trained ...</td>\n","      <td>...</td>\n","      <td>Since our goal is to train a GAN conditioned o...</td>\n","      <td>8e93efe94dcd8d5de9c5efc7961c2448a6759b8c</td>\n","      <td>Wav2Pix: Speech-conditioned Face Generation Us...</td>\n","      <td>Speech is a rich biometric signal that contain...</td>\n","      <td>2019.0</td>\n","      <td>31.0</td>\n","      <td>67.0</td>\n","      <td>5.0</td>\n","      <td>2019-03-25</td>\n","      <td>[{'authorId': '1381525702', 'name': 'A. Duarte...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44cb08fd-6293-4b62-a422-e3d8d8bc4957')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-44cb08fd-6293-4b62-a422-e3d8d8bc4957 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-44cb08fd-6293-4b62-a422-e3d8d8bc4957');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3a0a9e72-eca3-400f-bd8a-de214b79d7b1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a0a9e72-eca3-400f-bd8a-de214b79d7b1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3a0a9e72-eca3-400f-bd8a-de214b79d7b1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["df_all.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDGk0GTsxOY5","executionInfo":{"status":"ok","timestamp":1701228136875,"user_tz":-420,"elapsed":4,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"931effd8-d77d-4cba-975c-67ec9c8c8eae"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['datasetId', 'corpusId', 'title_x', 'abstract_x', 'sectionHeaderList',\n","       'sectionHeaderDict', 'corpus_id', 'pdf_url', 'introduction', 'result',\n","       'conclusion', 'method', 'paperId', 'title_y', 'abstract_y', 'year',\n","       'referenceCount', 'citationCount', 'influentialCitationCount',\n","       'publicationDate', 'authors'],\n","      dtype='object')"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# rename columns\n","df_all = df_all.rename(columns={\"title_x\":\"titleText\", \"abstract_x\":\"abstractText\", \"title_y\": \"title\", \"abstract_y\": \"abstract\",\"pdf_url\":\"pdfUrl\"})"],"metadata":{"id":"L7kSKt3Jtt3V","executionInfo":{"status":"ok","timestamp":1701228137456,"user_tz":-420,"elapsed":583,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# drop duplicate and reorder columns\n","df_all = df_all[['datasetId', 'paperId', 'corpusId','title',\n","       'abstract', 'year', 'referenceCount', 'citationCount',\n","       'influentialCitationCount', 'publicationDate', 'authors',\n","       'pdfUrl', 'titleText', 'abstractText',\n","       'sectionHeaderList', 'sectionHeaderDict',\n","       'introduction', 'method', 'result', 'conclusion']]"],"metadata":{"id":"u4W0E-GdyHid","executionInfo":{"status":"ok","timestamp":1701228137946,"user_tz":-420,"elapsed":493,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# convert datatype\n","df_all['referenceCount'] = df_all['referenceCount'].astype('Int64')\n","df_all['citationCount'] = df_all['citationCount'].astype('Int64')\n","df_all['influentialCitationCount'] = df_all['influentialCitationCount'].astype('Int64')"],"metadata":{"id":"E2wxZo-ry0vm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_all.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"id":"6iokah8Lygvl","executionInfo":{"status":"ok","timestamp":1701228137947,"user_tz":-420,"elapsed":5,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"76d0d0af-9f48-4af4-d2ad-78764bd8ffea"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   datasetId                                   paperId  corpusId  \\\n","0          1  7f12bd8efc6791399abdc587a1ca4f52776e2b88  13292366   \n","1          1  f67afec4226aba674e786698b39b85b124945ddd  20527197   \n","2          1  1c60ac28884d2414efcf0eec90561fae2a377311    116505   \n","3          1  18a429184af3d181e2de53d60a38e0314c25db5b   4693065   \n","4          1  8e93efe94dcd8d5de9c5efc7961c2448a6759b8c  85498398   \n","\n","                                               title  \\\n","0  Neural Machine Translation with Supervised Att...   \n","1  Spatial Variational Auto-Encoding via Matrix-V...   \n","2             Few-Shot Adversarial Domain Adaptation   \n","3  Mix and Match Networks: Encoder-Decoder Alignm...   \n","4  Wav2Pix: Speech-conditioned Face Generation Us...   \n","\n","                                            abstract    year  referenceCount  \\\n","0  The attention mechanism is appealing for neura...  2016.0              32   \n","1  The key idea of variational auto-encoders (VAE...  2017.0              32   \n","2  This work provides a framework for addressing ...  2017.0              64   \n","3  We address the problem of image translation be...  2018.0              38   \n","4  Speech is a rich biometric signal that contain...  2019.0              31   \n","\n","   citationCount  influentialCitationCount publicationDate  \\\n","0            141                        12      2016-09-14   \n","1              4                         0      2017-05-18   \n","2            335                        40      2017-11-01   \n","3             32                         1      2018-04-06   \n","4             67                         5      2019-03-25   \n","\n","                                             authors  \\\n","0  [{'authorId': '2978364', 'name': 'Lemao Liu'},...   \n","1  [{'authorId': '8492168', 'name': 'Zhengyang Wa...   \n","2  [{'authorId': '2897426', 'name': 'Saeid Motiia...   \n","3  [{'authorId': '2107916962', 'name': 'Yaxing Wa...   \n","4  [{'authorId': '1381525702', 'name': 'A. Duarte...   \n","\n","                                          pdfUrl  \\\n","0  https://www.aclweb.org/anthology/C16-1291.pdf   \n","1         https://arxiv.org/pdf/1705.06821v1.pdf   \n","2         https://arxiv.org/pdf/1711.02536v1.pdf   \n","3         https://arxiv.org/pdf/1804.02199v1.pdf   \n","4         https://arxiv.org/pdf/1903.10195v1.pdf   \n","\n","                                           titleText  \\\n","0  Neural Machine Translation with Supervised Att...   \n","1  Spatial Variational Auto-Encoding via Matrix-V...   \n","2             Few-Shot Adversarial Domain Adaptation   \n","3  Mix and match networks: encoder-decoder alignm...   \n","4                                                NaN   \n","\n","                                        abstractText  \\\n","0  The attention mechanism is appealing for neura...   \n","1  The key idea of variational auto-encoders (VAE...   \n","2  This work provides a framework for addressing ...   \n","3  We address the problem of image translation be...   \n","4                                                NaN   \n","\n","                                   sectionHeaderList  \\\n","0  ['Introduction', 'Revisiting Neural Machine Tr...   \n","1  ['Introduction', 'Background and Related Work'...   \n","2  ['Introduction', 'Related work', 'Few-shot adv...   \n","3  ['Introduction', 'Train', 'Related Work', '.1....   \n","4  ['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...   \n","\n","                                   sectionHeaderDict  \\\n","0  [{'raw_header': 'Introduction', 'normalize_hea...   \n","1  [{'raw_header': 'Introduction', 'normalize_hea...   \n","2  [{'raw_header': 'Introduction', 'normalize_hea...   \n","3  [{'raw_header': 'Introduction', 'normalize_hea...   \n","4  [{'raw_header': 'INTRODUCTION', 'normalize_hea...   \n","\n","                                        introduction  \\\n","0  Neural Machine Translation (NMT) has achieved ...   \n","1  The mathematical and computational modeling of...   \n","2  As deep learning approaches have gained promin...   \n","3  Image-to-image translations (or simply image t...   \n","4  Audio and visual signals are the most common m...   \n","\n","                                              method  \\\n","0                                                NaN   \n","1                                                NaN   \n","2                                                NaN   \n","3  We compare the results of our mix and match ne...   \n","4  Since our goal is to train a GAN conditioned o...   \n","\n","                                              result  \\\n","0  We conducted experiments on two Chinese-to-Eng...   \n","1  In this section, we report experimental result...   \n","2  We present results using the Office dataset [4...   \n","3  To the best of our knowledge there is no exist...   \n","4  Model training: The Wav2Pix model was trained ...   \n","\n","                                          conclusion  \n","0  It has been shown that attention mechanism in ...  \n","1  In this work, we propose spatial VAEs for imag...  \n","2  We have introduced a deep model combining a cl...  \n","3  In this paper we introduce the problem of zero...  \n","4  In this work we introduced a simple yet effect...  "],"text/html":["\n","  <div id=\"df-867b2191-c232-4e23-b5d6-5cbb512bbe55\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datasetId</th>\n","      <th>paperId</th>\n","      <th>corpusId</th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>year</th>\n","      <th>referenceCount</th>\n","      <th>citationCount</th>\n","      <th>influentialCitationCount</th>\n","      <th>publicationDate</th>\n","      <th>authors</th>\n","      <th>pdfUrl</th>\n","      <th>titleText</th>\n","      <th>abstractText</th>\n","      <th>sectionHeaderList</th>\n","      <th>sectionHeaderDict</th>\n","      <th>introduction</th>\n","      <th>method</th>\n","      <th>result</th>\n","      <th>conclusion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>7f12bd8efc6791399abdc587a1ca4f52776e2b88</td>\n","      <td>13292366</td>\n","      <td>Neural Machine Translation with Supervised Att...</td>\n","      <td>The attention mechanism is appealing for neura...</td>\n","      <td>2016.0</td>\n","      <td>32</td>\n","      <td>141</td>\n","      <td>12</td>\n","      <td>2016-09-14</td>\n","      <td>[{'authorId': '2978364', 'name': 'Lemao Liu'},...</td>\n","      <td>https://www.aclweb.org/anthology/C16-1291.pdf</td>\n","      <td>Neural Machine Translation with Supervised Att...</td>\n","      <td>The attention mechanism is appealing for neura...</td>\n","      <td>['Introduction', 'Revisiting Neural Machine Tr...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>Neural Machine Translation (NMT) has achieved ...</td>\n","      <td>NaN</td>\n","      <td>We conducted experiments on two Chinese-to-Eng...</td>\n","      <td>It has been shown that attention mechanism in ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>f67afec4226aba674e786698b39b85b124945ddd</td>\n","      <td>20527197</td>\n","      <td>Spatial Variational Auto-Encoding via Matrix-V...</td>\n","      <td>The key idea of variational auto-encoders (VAE...</td>\n","      <td>2017.0</td>\n","      <td>32</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2017-05-18</td>\n","      <td>[{'authorId': '8492168', 'name': 'Zhengyang Wa...</td>\n","      <td>https://arxiv.org/pdf/1705.06821v1.pdf</td>\n","      <td>Spatial Variational Auto-Encoding via Matrix-V...</td>\n","      <td>The key idea of variational auto-encoders (VAE...</td>\n","      <td>['Introduction', 'Background and Related Work'...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>The mathematical and computational modeling of...</td>\n","      <td>NaN</td>\n","      <td>In this section, we report experimental result...</td>\n","      <td>In this work, we propose spatial VAEs for imag...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1c60ac28884d2414efcf0eec90561fae2a377311</td>\n","      <td>116505</td>\n","      <td>Few-Shot Adversarial Domain Adaptation</td>\n","      <td>This work provides a framework for addressing ...</td>\n","      <td>2017.0</td>\n","      <td>64</td>\n","      <td>335</td>\n","      <td>40</td>\n","      <td>2017-11-01</td>\n","      <td>[{'authorId': '2897426', 'name': 'Saeid Motiia...</td>\n","      <td>https://arxiv.org/pdf/1711.02536v1.pdf</td>\n","      <td>Few-Shot Adversarial Domain Adaptation</td>\n","      <td>This work provides a framework for addressing ...</td>\n","      <td>['Introduction', 'Related work', 'Few-shot adv...</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>As deep learning approaches have gained promin...</td>\n","      <td>NaN</td>\n","      <td>We present results using the Office dataset [4...</td>\n","      <td>We have introduced a deep model combining a cl...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>18a429184af3d181e2de53d60a38e0314c25db5b</td>\n","      <td>4693065</td>\n","      <td>Mix and Match Networks: Encoder-Decoder Alignm...</td>\n","      <td>We address the problem of image translation be...</td>\n","      <td>2018.0</td>\n","      <td>38</td>\n","      <td>32</td>\n","      <td>1</td>\n","      <td>2018-04-06</td>\n","      <td>[{'authorId': '2107916962', 'name': 'Yaxing Wa...</td>\n","      <td>https://arxiv.org/pdf/1804.02199v1.pdf</td>\n","      <td>Mix and match networks: encoder-decoder alignm...</td>\n","      <td>We address the problem of image translation be...</td>\n","      <td>['Introduction', 'Train', 'Related Work', '.1....</td>\n","      <td>[{'raw_header': 'Introduction', 'normalize_hea...</td>\n","      <td>Image-to-image translations (or simply image t...</td>\n","      <td>We compare the results of our mix and match ne...</td>\n","      <td>To the best of our knowledge there is no exist...</td>\n","      <td>In this paper we introduce the problem of zero...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>8e93efe94dcd8d5de9c5efc7961c2448a6759b8c</td>\n","      <td>85498398</td>\n","      <td>Wav2Pix: Speech-conditioned Face Generation Us...</td>\n","      <td>Speech is a rich biometric signal that contain...</td>\n","      <td>2019.0</td>\n","      <td>31</td>\n","      <td>67</td>\n","      <td>5</td>\n","      <td>2019-03-25</td>\n","      <td>[{'authorId': '1381525702', 'name': 'A. Duarte...</td>\n","      <td>https://arxiv.org/pdf/1903.10195v1.pdf</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>['INTRODUCTION', 'RELATED WORKS', 'Real/Fake',...</td>\n","      <td>[{'raw_header': 'INTRODUCTION', 'normalize_hea...</td>\n","      <td>Audio and visual signals are the most common m...</td>\n","      <td>Since our goal is to train a GAN conditioned o...</td>\n","      <td>Model training: The Wav2Pix model was trained ...</td>\n","      <td>In this work we introduced a simple yet effect...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-867b2191-c232-4e23-b5d6-5cbb512bbe55')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-867b2191-c232-4e23-b5d6-5cbb512bbe55 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-867b2191-c232-4e23-b5d6-5cbb512bbe55');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cabf3268-7e36-4dcb-bf17-0985b9d46cad\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cabf3268-7e36-4dcb-bf17-0985b9d46cad')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cabf3268-7e36-4dcb-bf17-0985b9d46cad button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["df_all.dtypes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dg7CU5KaylR6","executionInfo":{"status":"ok","timestamp":1701228137947,"user_tz":-420,"elapsed":4,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"dcf74463-bee9-4898-9256-89e99f4f9e32"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["datasetId                     int64\n","paperId                      object\n","corpusId                      int64\n","title                        object\n","abstract                     object\n","year                        float64\n","referenceCount                Int64\n","citationCount                 Int64\n","influentialCitationCount      Int64\n","publicationDate              object\n","authors                      object\n","pdfUrl                       object\n","titleText                    object\n","abstractText                 object\n","sectionHeaderList            object\n","sectionHeaderDict            object\n","introduction                 object\n","method                       object\n","result                       object\n","conclusion                   object\n","dtype: object"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["### Binning Citation Count and Label Encoding"],"metadata":{"id":"AXDw51O9BoxD"}},{"cell_type":"code","source":["# distribution of numerical columns\n","pd.options.display.float_format = '{:.2f}'.format\n","df_all.describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99,0.995, 0.999, 0.9999])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"id":"1b4Cflz-CoZf","executionInfo":{"status":"ok","timestamp":1701228522513,"user_tz":-420,"elapsed":7,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"7404ffe2-a72e-436f-e4b6-b19bb1af7f9c"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        datasetId     corpusId      year  referenceCount  citationCount  \\\n","count   171124.00    171124.00 171074.00       171074.00      171074.00   \n","mean        15.48 183783331.56   2019.82           31.39          33.42   \n","std          8.66  84963288.98      1.89           20.77         497.27   \n","min          1.00      2506.00   2015.00            0.00           0.00   \n","10%          3.00  15645935.40   2017.00           10.00           0.00   \n","20%          7.00  70223324.60   2018.00           15.00           0.00   \n","30%          9.00 201645429.00   2019.00           19.00           1.00   \n","40%         12.00 213382141.00   2020.00           23.00           2.00   \n","50%         15.00 220871196.00   2020.00           28.00           4.00   \n","60%         18.00 229457088.60   2020.00           33.00           7.00   \n","70%         21.00 236087620.70   2021.00           39.00          11.00   \n","80%         25.00 246441833.00   2022.00           46.00          21.00   \n","90%         28.00 252967696.40   2022.00           57.00          51.00   \n","95%         29.00 257632515.55   2023.00           68.00         105.00   \n","99%         30.00 259702068.16   2023.00           94.00         437.00   \n","99.5%       30.00 260353478.42   2023.00          107.00         729.00   \n","99.9%       30.00 262020909.65   2023.00          148.00        2399.42   \n","99.99%      30.00 263124558.24   2023.00          275.79       11503.84   \n","max         30.00 263533069.00   2023.00         1611.00      144247.00   \n","\n","        influentialCitationCount  \n","count                  171074.00  \n","mean                        4.57  \n","std                       102.33  \n","min                         0.00  \n","10%                         0.00  \n","20%                         0.00  \n","30%                         0.00  \n","40%                         0.00  \n","50%                         0.00  \n","60%                         0.00  \n","70%                         1.00  \n","80%                         2.00  \n","90%                         6.00  \n","95%                        13.00  \n","99%                        65.00  \n","99.5%                     117.64  \n","99.9%                     422.00  \n","99.99%                   2214.27  \n","max                     30732.00  "],"text/html":["\n","  <div id=\"df-1fe20791-34bb-42cf-8650-a341fb4a7fc1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datasetId</th>\n","      <th>corpusId</th>\n","      <th>year</th>\n","      <th>referenceCount</th>\n","      <th>citationCount</th>\n","      <th>influentialCitationCount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>171124.00</td>\n","      <td>171124.00</td>\n","      <td>171074.00</td>\n","      <td>171074.00</td>\n","      <td>171074.00</td>\n","      <td>171074.00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>15.48</td>\n","      <td>183783331.56</td>\n","      <td>2019.82</td>\n","      <td>31.39</td>\n","      <td>33.42</td>\n","      <td>4.57</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>8.66</td>\n","      <td>84963288.98</td>\n","      <td>1.89</td>\n","      <td>20.77</td>\n","      <td>497.27</td>\n","      <td>102.33</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.00</td>\n","      <td>2506.00</td>\n","      <td>2015.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>10%</th>\n","      <td>3.00</td>\n","      <td>15645935.40</td>\n","      <td>2017.00</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>20%</th>\n","      <td>7.00</td>\n","      <td>70223324.60</td>\n","      <td>2018.00</td>\n","      <td>15.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>30%</th>\n","      <td>9.00</td>\n","      <td>201645429.00</td>\n","      <td>2019.00</td>\n","      <td>19.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>40%</th>\n","      <td>12.00</td>\n","      <td>213382141.00</td>\n","      <td>2020.00</td>\n","      <td>23.00</td>\n","      <td>2.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>15.00</td>\n","      <td>220871196.00</td>\n","      <td>2020.00</td>\n","      <td>28.00</td>\n","      <td>4.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>60%</th>\n","      <td>18.00</td>\n","      <td>229457088.60</td>\n","      <td>2020.00</td>\n","      <td>33.00</td>\n","      <td>7.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>70%</th>\n","      <td>21.00</td>\n","      <td>236087620.70</td>\n","      <td>2021.00</td>\n","      <td>39.00</td>\n","      <td>11.00</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>80%</th>\n","      <td>25.00</td>\n","      <td>246441833.00</td>\n","      <td>2022.00</td>\n","      <td>46.00</td>\n","      <td>21.00</td>\n","      <td>2.00</td>\n","    </tr>\n","    <tr>\n","      <th>90%</th>\n","      <td>28.00</td>\n","      <td>252967696.40</td>\n","      <td>2022.00</td>\n","      <td>57.00</td>\n","      <td>51.00</td>\n","      <td>6.00</td>\n","    </tr>\n","    <tr>\n","      <th>95%</th>\n","      <td>29.00</td>\n","      <td>257632515.55</td>\n","      <td>2023.00</td>\n","      <td>68.00</td>\n","      <td>105.00</td>\n","      <td>13.00</td>\n","    </tr>\n","    <tr>\n","      <th>99%</th>\n","      <td>30.00</td>\n","      <td>259702068.16</td>\n","      <td>2023.00</td>\n","      <td>94.00</td>\n","      <td>437.00</td>\n","      <td>65.00</td>\n","    </tr>\n","    <tr>\n","      <th>99.5%</th>\n","      <td>30.00</td>\n","      <td>260353478.42</td>\n","      <td>2023.00</td>\n","      <td>107.00</td>\n","      <td>729.00</td>\n","      <td>117.64</td>\n","    </tr>\n","    <tr>\n","      <th>99.9%</th>\n","      <td>30.00</td>\n","      <td>262020909.65</td>\n","      <td>2023.00</td>\n","      <td>148.00</td>\n","      <td>2399.42</td>\n","      <td>422.00</td>\n","    </tr>\n","    <tr>\n","      <th>99.99%</th>\n","      <td>30.00</td>\n","      <td>263124558.24</td>\n","      <td>2023.00</td>\n","      <td>275.79</td>\n","      <td>11503.84</td>\n","      <td>2214.27</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>30.00</td>\n","      <td>263533069.00</td>\n","      <td>2023.00</td>\n","      <td>1611.00</td>\n","      <td>144247.00</td>\n","      <td>30732.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fe20791-34bb-42cf-8650-a341fb4a7fc1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1fe20791-34bb-42cf-8650-a341fb4a7fc1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1fe20791-34bb-42cf-8650-a341fb4a7fc1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-999224eb-993e-463b-9fb0-06383981810f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-999224eb-993e-463b-9fb0-06383981810f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-999224eb-993e-463b-9fb0-06383981810f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# remove outlier at percentile 0.999\n","upper_limit = df_all['citationCount'].quantile(0.999)\n","print(upper_limit)\n","df_rm_outlier = df_all[(df_all['citationCount']<=upper_limit)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzuTYmdjDCn5","executionInfo":{"status":"ok","timestamp":1701228522513,"user_tz":-420,"elapsed":5,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"403bfe08-f0f9-462e-abc9-f4f9d3ee61a2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2399.416000000201\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# plot bar chart\n","plt.hist(df_rm_outlier['citationCount'], bins=100, color='gray', edgecolor='black')\n","plt.xlabel('Citation Count')\n","plt.ylabel('Frequency')\n","plt.title('Histogram of Citation Count')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"U0J1A8aPC2cw","executionInfo":{"status":"ok","timestamp":1701228523571,"user_tz":-420,"elapsed":1061,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"7bc3391c-d1a3-4cb0-cde2-5897eef755ba"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["df_rm_outlier['citationCount'].describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uplkNvxyDRwF","executionInfo":{"status":"ok","timestamp":1701228627797,"user_tz":-420,"elapsed":6,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"e15b047e-3e2d-4621-a485-a5c7c4f7b208"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count   170902.00\n","mean        25.85\n","std         99.05\n","min          0.00\n","25%          1.00\n","50%          4.00\n","75%         15.00\n","max       2392.00\n","Name: citationCount, dtype: float64"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# bin ['[0, 10)', '[10, 100)', '[100, +inf)']\n","bins = [0, 10, 100, np.inf]\n","\n","# label encoding to multi-class labels\n","labels = ['0', '1', '2']\n","\n","# bin the data and count occurrences in each bin\n","df_rm_outlier['citationCountBin'] = pd.cut(df_rm_outlier['citationCount'], bins=bins, labels=labels, right=False)\n","bin_counts = df_rm_outlier['citationCountBin'].value_counts(sort=False)\n","bin_counts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3V2AvHaDUhU","executionInfo":{"status":"ok","timestamp":1701228629387,"user_tz":-420,"elapsed":2,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"160bb159-6413-4dae-8f90-8df07b33a8f1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-9b1d9000b66b>:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_rm_outlier['citationCountBin'] = pd.cut(df_rm_outlier['citationCount'], bins=bins, labels=labels, right=False)\n"]},{"output_type":"execute_result","data":{"text/plain":["0    114451\n","1     47598\n","2      8853\n","Name: citationCountBin, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# plot bar chart of citationCountRange (Binned)\n","bin_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n","plt.xlabel('Citation Count Range')\n","plt.ylabel('Frequency')\n","plt.title('Citation Count Range and Frequency')\n","plt.xticks(rotation=0, ha='center')\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"N-Q3bvNtDa0O","executionInfo":{"status":"ok","timestamp":1701228523572,"user_tz":-420,"elapsed":10,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}},"outputId":"fa2254fe-b2d4-48c3-e845-9709874a6720"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# write final input for modeling to file\n","df_rm_outlier.to_csv(base_path + 'filtered_corpusid_input.csv', index=False)"],"metadata":{"id":"uWYgx1PYERmc","executionInfo":{"status":"ok","timestamp":1701228627797,"user_tz":-420,"elapsed":104234,"user":{"displayName":"Watchareepast Techanitipat","userId":"11454423977715884194"}}},"execution_count":9,"outputs":[]}]}